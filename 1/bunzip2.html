<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <link rel="stylesheet" href="../style.css" type="text/css" media="all"/>
  <title>bzip2(1)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">bzip2(1)</td>
    <td class="head-vol"><a href=".">General Commands Manual</a></td>
    <td class="head-rtitle">bzip2(1)</td>
  </tr>
</table>
<div class="manual-text">
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
<p class="Pp">bzip2, bunzip2 - a block-sorting file compressor, v1.0.8
  <br/>
  bzcat - decompresses files to stdout
  <br/>
  bzip2recover - recovers data from damaged bzip2 files</p>
<p class="Pp"></p>
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<p class="Pp"><b>bzip2</b> [<b> -cdfkqstvzVL123456789 </b>] [ <i>filenames
    ...</i> ]
  <br/>
  <b>bunzip2</b> [<b> -fkvsVL </b>] [ <i>filenames ...</i> ]
  <br/>
  <b>bzcat</b> [<b> -s </b>] [ <i>filenames ...</i> ]
  <br/>
  <b>bzip2recover</b> <i>filename</i></p>
<p class="Pp"></p>
</section>
<section class="Sh">
<h1 class="Sh" id="DESCRIPTION"><a class="permalink" href="#DESCRIPTION">DESCRIPTION</a></h1>
<p class="Pp"><i>bzip2</i> compresses files using the Burrows-Wheeler block
    sorting text compression algorithm, and Huffman coding. Compression is
    generally considerably better than that achieved by more conventional
    LZ77/LZ78-based compressors, and approaches the performance of the PPM
    family of statistical compressors.</p>
<p class="Pp">The command-line options are deliberately very similar to those of
    <i>GNU gzip,</i> but they are not identical.</p>
<p class="Pp"><i>bzip2</i> expects a list of file names to accompany the
    command-line flags. Each file is replaced by a compressed version of itself,
    with the name &quot;original_name.bz2&quot;. Each compressed file has the
    same modification date, permissions, and, when possible, ownership as the
    corresponding original, so that these properties can be correctly restored
    at decompression time. File name handling is naive in the sense that there
    is no mechanism for preserving original file names, permissions, ownerships
    or dates in filesystems which lack these concepts, or have serious file name
    length restrictions, such as MS-DOS.</p>
<p class="Pp"><i>bzip2</i> and <i>bunzip2</i> will by default not overwrite
    existing files. If you want this to happen, specify the -f flag.</p>
<p class="Pp">If no file names are specified, <i>bzip2</i> compresses from
    standard input to standard output. In this case, <i>bzip2</i> will decline
    to write compressed output to a terminal, as this would be entirely
    incomprehensible and therefore pointless.</p>
<p class="Pp"><i>bunzip2</i> (or <i>bzip2 -d)</i> decompresses all specified
    files. Files which were not created by <i>bzip2</i> will be detected and
    ignored, and a warning issued. <i>bzip2</i> attempts to guess the filename
    for the decompressed file from that of the compressed file as follows:</p>
<p class="Pp">
  <br/>
   filename.bz2 becomes filename
  <br/>
   filename.bz becomes filename
  <br/>
   filename.tbz2 becomes filename.tar
  <br/>
   filename.tbz becomes filename.tar
  <br/>
   anyothername becomes anyothername.out</p>
<p class="Pp">If the file does not end in one of the recognised endings,
    <i>.bz2,</i> <i>.bz,</i> <i>.tbz2</i> or <i>.tbz,</i> <i>bzip2</i> complains
    that it cannot guess the name of the original file, and uses the original
    name with <i>.out</i> appended.</p>
<p class="Pp">As with compression, supplying no filenames causes decompression
    from standard input to standard output.</p>
<p class="Pp"><i>bunzip2</i> will correctly decompress a file which is the
    concatenation of two or more compressed files. The result is the
    concatenation of the corresponding uncompressed files. Integrity testing
    (-t) of concatenated compressed files is also supported.</p>
<p class="Pp">You can also compress or decompress files to the standard output
    by giving the -c flag. Multiple files may be compressed and decompressed
    like this. The resulting outputs are fed sequentially to stdout. Compression
    of multiple files in this manner generates a stream containing multiple
    compressed file representations. Such a stream can be decompressed correctly
    only by <i>bzip2</i> version 0.9.0 or later. Earlier versions of
    <i>bzip2</i> will stop after decompressing the first file in the stream.</p>
<p class="Pp"><i>bzcat</i> (or <i>bzip2 -dc)</i> decompresses all specified
    files to the standard output.</p>
<p class="Pp"><i>bzip2</i> will read arguments from the environment variables
    <i>BZIP2</i> and <i>BZIP,</i> in that order, and will process them before
    any arguments read from the command line. This gives a convenient way to
    supply default arguments.</p>
<p class="Pp">Compression is always performed, even if the compressed file is
    slightly larger than the original. Files of less than about one hundred
    bytes tend to get larger, since the compression mechanism has a constant
    overhead in the region of 50 bytes. Random data (including the output of
    most file compressors) is coded at about 8.05 bits per byte, giving an
    expansion of around 0.5%.</p>
<p class="Pp">As a self-check for your protection, <i>bzip2</i> uses 32-bit CRCs
    to make sure that the decompressed version of a file is identical to the
    original. This guards against corruption of the compressed data, and against
    undetected bugs in <i>bzip2</i> (hopefully very unlikely). The chances of
    data corruption going undetected is microscopic, about one chance in four
    billion for each file processed. Be aware, though, that the check occurs
    upon decompression, so it can only tell you that something is wrong. It
    can't help you recover the original uncompressed data. You can use
    <i>bzip2recover</i> to try to recover data from damaged files.</p>
<p class="Pp">Return values: 0 for a normal exit, 1 for environmental problems
    (file not found, invalid flags, I/O errors, &amp;c), 2 to indicate a corrupt
    compressed file, 3 for an internal consistency error (eg, bug) which caused
    <i>bzip2</i> to panic.</p>
<p class="Pp"></p>
</section>
<section class="Sh">
<h1 class="Sh" id="OPTIONS"><a class="permalink" href="#OPTIONS">OPTIONS</a></h1>
<dl class="Bl-tag">
  <dt id="c"><a class="permalink" href="#c"><b>-c --stdout</b></a></dt>
  <dd>Compress or decompress to standard output.</dd>
  <dt id="d"><a class="permalink" href="#d"><b>-d --decompress</b></a></dt>
  <dd>Force decompression. <i>bzip2,</i> <i>bunzip2</i> and <i>bzcat</i> are
      really the same program, and the decision about what actions to take is
      done on the basis of which name is used. This flag overrides that
      mechanism, and forces <i>bzip2</i> to decompress.</dd>
  <dt id="z"><a class="permalink" href="#z"><b>-z --compress</b></a></dt>
  <dd>The complement to -d: forces compression, regardless of the invocation
      name.</dd>
  <dt id="t"><a class="permalink" href="#t"><b>-t --test</b></a></dt>
  <dd>Check integrity of the specified file(s), but don't decompress them. This
      really performs a trial decompression and throws away the result.</dd>
  <dt id="f"><a class="permalink" href="#f"><b>-f --force</b></a></dt>
  <dd>Force overwrite of output files. Normally, <i>bzip2</i> will not overwrite
      existing output files. Also forces <i>bzip2</i> to break hard links to
      files, which it otherwise wouldn't do.
    <p class="Pp">bzip2 normally declines to decompress files which don't have
        the correct magic header bytes. If forced (-f), however, it will pass
        such files through unmodified. This is how GNU gzip behaves.</p>
  </dd>
  <dt id="k"><a class="permalink" href="#k"><b>-k --keep</b></a></dt>
  <dd>Keep (don't delete) input files during compression or decompression.</dd>
  <dt id="s"><a class="permalink" href="#s"><b>-s --small</b></a></dt>
  <dd>Reduce memory usage, for compression, decompression and testing. Files are
      decompressed and tested using a modified algorithm which only requires 2.5
      bytes per block byte. This means any file can be decompressed in 2300k of
      memory, albeit at about half the normal speed.
    <p class="Pp">During compression, -s selects a block size of 200k, which
        limits memory use to around the same figure, at the expense of your
        compression ratio. In short, if your machine is low on memory (8
        megabytes or less), use -s for everything. See MEMORY MANAGEMENT
      below.</p>
  </dd>
  <dt id="q"><a class="permalink" href="#q"><b>-q --quiet</b></a></dt>
  <dd>Suppress non-essential warning messages. Messages pertaining to I/O errors
      and other critical events will not be suppressed.</dd>
  <dt id="v"><a class="permalink" href="#v"><b>-v --verbose</b></a></dt>
  <dd>Verbose mode -- show the compression ratio for each file processed.
      Further -v's increase the verbosity level, spewing out lots of information
      which is primarily of interest for diagnostic purposes.</dd>
  <dt id="L"><a class="permalink" href="#L"><b>-L --license -V
    --version</b></a></dt>
  <dd>Display the software version, license terms and conditions.</dd>
  <dt><b>-1 (or --fast) to -9 (or --best)</b></dt>
  <dd>Set the block size to 100 k, 200 k .. 900 k when compressing. Has no
      effect when decompressing. See MEMORY MANAGEMENT below. The --fast and
      --best aliases are primarily for GNU gzip compatibility. In particular,
      --fast doesn't make things significantly faster. And --best merely selects
      the default behaviour.</dd>
  <dt><b>--</b></dt>
  <dd>Treats all subsequent arguments as file names, even if they start with a
      dash. This is so you can handle files with names beginning with a dash,
      for example: bzip2 -- -myfilename.</dd>
  <dt id="repetitive-fast"><a class="permalink" href="#repetitive-fast"><b>--repetitive-fast
    --repetitive-best</b></a></dt>
  <dd>These flags are redundant in versions 0.9.5 and above. They provided some
      coarse control over the behaviour of the sorting algorithm in earlier
      versions, which was sometimes useful. 0.9.5 and above have an improved
      algorithm which renders these flags irrelevant.
    <p class="Pp"></p>
  </dd>
</dl>
</section>
<section class="Sh">
<h1 class="Sh" id="MEMORY_MANAGEMENT"><a class="permalink" href="#MEMORY_MANAGEMENT">MEMORY
  MANAGEMENT</a></h1>
<p class="Pp"><i>bzip2</i> compresses large files in blocks. The block size
    affects both the compression ratio achieved, and the amount of memory needed
    for compression and decompression. The flags -1 through -9 specify the block
    size to be 100,000 bytes through 900,000 bytes (the default) respectively.
    At decompression time, the block size used for compression is read from the
    header of the compressed file, and <i>bunzip2</i> then allocates itself just
    enough memory to decompress the file. Since block sizes are stored in
    compressed files, it follows that the flags -1 to -9 are irrelevant to and
    so ignored during decompression.</p>
<p class="Pp">Compression and decompression requirements, in bytes, can be
    estimated as:</p>
<p class="Pp">
  <br/>
   Compression: 400k + ( 8 x block size )</p>
<p class="Pp">
  <br/>
   Decompression: 100k + ( 4 x block size ), or
  <br/>
   100k + ( 2.5 x block size )</p>
<p class="Pp">Larger block sizes give rapidly diminishing marginal returns. Most
    of the compression comes from the first two or three hundred k of block
    size, a fact worth bearing in mind when using <i>bzip2</i> on small
    machines. It is also important to appreciate that the decompression memory
    requirement is set at compression time by the choice of block size.</p>
<p class="Pp">For files compressed with the default 900k block size,
    <i>bunzip2</i> will require about 3700 kbytes to decompress. To support
    decompression of any file on a 4 megabyte machine, <i>bunzip2</i> has an
    option to decompress using approximately half this amount of memory, about
    2300 kbytes. Decompression speed is also halved, so you should use this
    option only where necessary. The relevant flag is -s.</p>
<p class="Pp">In general, try and use the largest block size memory constraints
    allow, since that maximises the compression achieved. Compression and
    decompression speed are virtually unaffected by block size.</p>
<p class="Pp">Another significant point applies to files which fit in a single
    block -- that means most files you'd encounter using a large block size. The
    amount of real memory touched is proportional to the size of the file, since
    the file is smaller than a block. For example, compressing a file 20,000
    bytes long with the flag -9 will cause the compressor to allocate around
    7600k of memory, but only touch 400k + 20000 * 8 = 560 kbytes of it.
    Similarly, the decompressor will allocate 3700k but only touch 100k + 20000
    * 4 = 180 kbytes.</p>
<p class="Pp">Here is a table which summarises the maximum memory usage for
    different block sizes. Also recorded is the total compressed size for 14
    files of the Calgary Text Compression Corpus totalling 3,141,622 bytes. This
    column gives some feel for how compression varies with block size. These
    figures tend to understate the advantage of larger block sizes for larger
    files, since the Corpus is dominated by smaller files.</p>
<p class="Pp">
  <br/>
   Compress Decompress Decompress Corpus
  <br/>
   Flag usage usage -s usage Size</p>
<p class="Pp">
  <br/>
   -1 1200k 500k 350k 914704
  <br/>
   -2 2000k 900k 600k 877703
  <br/>
   -3 2800k 1300k 850k 860338
  <br/>
   -4 3600k 1700k 1100k 846899
  <br/>
   -5 4400k 2100k 1350k 845160
  <br/>
   -6 5200k 2500k 1600k 838626
  <br/>
   -7 6100k 2900k 1850k 834096
  <br/>
   -8 6800k 3300k 2100k 828642
  <br/>
   -9 7600k 3700k 2350k 828642</p>
<p class="Pp"></p>
</section>
<section class="Sh">
<h1 class="Sh" id="RECOVERING_DATA_FROM_DAMAGED_FILES"><a class="permalink" href="#RECOVERING_DATA_FROM_DAMAGED_FILES">RECOVERING
  DATA FROM DAMAGED FILES</a></h1>
<p class="Pp"><i>bzip2</i> compresses files in blocks, usually 900kbytes long.
    Each block is handled independently. If a media or transmission error causes
    a multi-block .bz2 file to become damaged, it may be possible to recover
    data from the undamaged blocks in the file.</p>
<p class="Pp">The compressed representation of each block is delimited by a
    48-bit pattern, which makes it possible to find the block boundaries with
    reasonable certainty. Each block also carries its own 32-bit CRC, so damaged
    blocks can be distinguished from undamaged ones.</p>
<p class="Pp"><i>bzip2recover</i> is a simple program whose purpose is to search
    for blocks in .bz2 files, and write each block out into its own .bz2 file.
    You can then use <i>bzip2</i> -t to test the integrity of the resulting
    files, and decompress those which are undamaged.</p>
<p class="Pp"><i>bzip2recover</i> takes a single argument, the name of the
    damaged file, and writes a number of files &quot;rec00001file.bz2&quot;,
    &quot;rec00002file.bz2&quot;, etc, containing the extracted blocks. The
    output filenames are designed so that the use of wildcards in subsequent
    processing -- for example, &quot;bzip2 -dc rec*file.bz2 &gt;
    recovered_data&quot; -- processes the files in the correct order.</p>
<p class="Pp"><i>bzip2recover</i> should be of most use dealing with large .bz2
    files, as these will contain many blocks. It is clearly futile to use it on
    damaged single-block files, since a damaged block cannot be recovered. If
    you wish to minimise any potential data loss through media or transmission
    errors, you might consider compressing with a smaller block size.</p>
<p class="Pp"></p>
</section>
<section class="Sh">
<h1 class="Sh" id="PERFORMANCE_NOTES"><a class="permalink" href="#PERFORMANCE_NOTES">PERFORMANCE
  NOTES</a></h1>
<p class="Pp">The sorting phase of compression gathers together similar strings
    in the file. Because of this, files containing very long runs of repeated
    symbols, like &quot;aabaabaabaab ...&quot; (repeated several hundred times)
    may compress more slowly than normal. Versions 0.9.5 and above fare much
    better than previous versions in this respect. The ratio between worst-case
    and average-case compression time is in the region of 10:1. For previous
    versions, this figure was more like 100:1. You can use the -vvvv option to
    monitor progress in great detail, if you want.</p>
<p class="Pp">Decompression speed is unaffected by these phenomena.</p>
<p class="Pp"><i>bzip2</i> usually allocates several megabytes of memory to
    operate in, and then charges all over it in a fairly random fashion. This
    means that performance, both for compressing and decompressing, is largely
    determined by the speed at which your machine can service cache misses.
    Because of this, small changes to the code to reduce the miss rate have been
    observed to give disproportionately large performance improvements. I
    imagine <i>bzip2</i> will perform best on machines with very large
  caches.</p>
<p class="Pp"></p>
</section>
<section class="Sh">
<h1 class="Sh" id="CAVEATS"><a class="permalink" href="#CAVEATS">CAVEATS</a></h1>
<p class="Pp">I/O error messages are not as helpful as they could be.
    <i>bzip2</i> tries hard to detect I/O errors and exit cleanly, but the
    details of what the problem is sometimes seem rather misleading.</p>
<p class="Pp">This manual page pertains to version 1.0.8 of <i>bzip2.</i>
    Compressed data created by this version is entirely forwards and backwards
    compatible with the previous public releases, versions 0.1pl2, 0.9.0, 0.9.5,
    1.0.0, 1.0.1, 1.0.2 and above, but with the following exception: 0.9.0 and
    above can correctly decompress multiple concatenated compressed files.
    0.1pl2 cannot do this; it will stop after decompressing just the first file
    in the stream.</p>
<p class="Pp"><i>bzip2recover</i> versions prior to 1.0.2 used 32-bit integers
    to represent bit positions in compressed files, so they could not handle
    compressed files more than 512 megabytes long. Versions 1.0.2 and above use
    64-bit ints on some platforms which support them (GNU supported targets, and
    Windows). To establish whether or not bzip2recover was built with such a
    limitation, run it without arguments. In any event you can build yourself an
    unlimited version if you can recompile it with MaybeUInt64 set to be an
    unsigned 64-bit integer.</p>
<p class="Pp"></p>
<p class="Pp"></p>
<p class="Pp"></p>
</section>
<section class="Sh">
<h1 class="Sh" id="AUTHOR"><a class="permalink" href="#AUTHOR">AUTHOR</a></h1>
<p class="Pp">Julian Seward, jseward@acm.org.</p>
<p class="Pp">https://sourceware.org/bzip2/</p>
<p class="Pp">The ideas embodied in <i>bzip2</i> are due to (at least) the
    following people: Michael Burrows and David Wheeler (for the block sorting
    transformation), David Wheeler (again, for the Huffman coder), Peter Fenwick
    (for the structured coding model in the original <i>bzip,</i> and many
    refinements), and Alistair Moffat, Radford Neal and Ian Witten (for the
    arithmetic coder in the original <i>bzip).</i> I am much indebted for their
    help, support and advice. See the manual in the source distribution for
    pointers to sources of documentation. Christian von Roques encouraged me to
    look for faster sorting algorithms, so as to speed up compression. Bela
    Lubkin encouraged me to improve the worst-case compression performance.
    Donna Robinson XMLised the documentation. The bz* scripts are derived from
    those of GNU gzip. Many people sent patches, helped with portability
    problems, lent machines, gave advice and were generally helpful.</p>
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date"></td>
    <td class="foot-os"><a href=".."></a></td>
  </tr>
</table>
</body>
</html>
