<!DOCTYPE html>
<html>
<!-- This is an automatically generated file.  Do not edit.
   -*- nroff -*-
 -->
<head>
  <meta charset="utf-8"/>
  <link rel="stylesheet" href="../style.css" type="text/css" media="all"/>
  <title>MPSCNNBinaryKernel(3)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">MPSCNNBinaryKernel(3)</td>
    <td class="head-vol">MetalPerformanceShaders.framework</td>
    <td class="head-rtitle">MPSCNNBinaryKernel(3)</td>
  </tr>
</table>
<div class="manual-text">
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
<p class="Pp">MPSCNNBinaryKernel</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<p class="Pp">#import &lt;MPSCNNKernel.h&gt;</p>
<p class="Pp">Inherits <b>MPSKernel</b>.</p>
<p class="Pp">Inherited by <b>MPSCNNArithmetic</b>, <b>MPSCNNGradientKernel</b>,
    and <b>MPSNNReduceBinary</b>.</p>
<section class="Ss">
<h2 class="Ss" id="Instance_Methods"><a class="permalink" href="#Instance_Methods">Instance
  Methods</a></h2>
<br/>
<p class="Pp">(nonnull instancetype) - <b>initWithDevice:</b>
  <br/>
  (nullable instancetype) - <b>initWithCoder:device:</b>
  <br/>
  (void) -
    <b>encodeToCommandBuffer:primaryImage:secondaryImage:destinationImage:</b>
  <br/>
  (void) -
    <b>encodeBatchToCommandBuffer:primaryImages:secondaryImages:destinationImages:</b>
  <br/>
  (<b>MPSImage</b> *__nonnull) -
    <b>encodeToCommandBuffer:primaryImage:secondaryImage:</b>
  <br/>
  (<b>MPSImageBatch</b> *__nonnull) -
    <b>encodeBatchToCommandBuffer:primaryImages:secondaryImages:</b>
  <br/>
  (<b>MPSImage</b> *__nonnull) -
    <b>encodeToCommandBuffer:primaryImage:secondaryImage:destinationState:destinationStateIsTemporary:</b>
  <br/>
  (<b>MPSImageBatch</b> *__nonnull) -
    <b>encodeBatchToCommandBuffer:primaryImages:secondaryImages:destinationStates:destinationStateIsTemporary:</b>
  <br/>
  (<b>MPSState</b> *__nullable) -
    <b>resultStateForPrimaryImage:secondaryImage:sourceStates:destinationImage:</b>
  <br/>
  (<b>MPSStateBatch</b> *__nullable) -
    <b>resultStateBatchForPrimaryImage:secondaryImage:sourceStates:destinationImage:</b>
  <br/>
  (<b>MPSState</b> *__nullable) -
    <b>temporaryResultStateForCommandBuffer:primaryImage:secondaryImage:sourceStates:destinationImage:</b>
  <br/>
  (<b>MPSStateBatch</b> *__nullable) -
    <b>temporaryResultStateBatchForCommandBuffer:primaryImage:secondaryImage:sourceStates:destinationImage:</b>
  <br/>
  (BOOL) - <b>isResultStateReusedAcrossBatch</b>
  <br/>
  (BOOL) - <b>appendBatchBarrier</b>
  <br/>
  (<b>MPSImageDescriptor</b> *__nonnull) -
    <b>destinationImageDescriptorForSourceImages:sourceStates:</b>
  <br/>
  <br/>
</p>
</section>
<section class="Ss">
<h2 class="Ss" id="Properties"><a class="permalink" href="#Properties">Properties</a></h2>
<br/>
<p class="Pp"><b>MPSOffset</b> <b>primaryOffset</b>
  <br/>
  <b>MPSOffset</b> <b>secondaryOffset</b>
  <br/>
  MTLRegion <b>clipRect</b>
  <br/>
  NSUInteger <b>destinationFeatureChannelOffset</b>
  <br/>
  NSUInteger <b>primarySourceFeatureChannelOffset</b>
  <br/>
  NSUInteger <b>secondarySourceFeatureChannelOffset</b>
  <br/>
  NSUInteger <b>primarySourceFeatureChannelMaxCount</b>
  <br/>
  NSUInteger <b>secondarySourceFeatureChannelMaxCount</b>
  <br/>
  <b>MPSImageEdgeMode</b> <b>primaryEdgeMode</b>
  <br/>
  <b>MPSImageEdgeMode</b> <b>secondaryEdgeMode</b>
  <br/>
  NSUInteger <b>primaryKernelWidth</b>
  <br/>
  NSUInteger <b>primaryKernelHeight</b>
  <br/>
  NSUInteger <b>secondaryKernelWidth</b>
  <br/>
  NSUInteger <b>secondaryKernelHeight</b>
  <br/>
  NSUInteger <b>primaryStrideInPixelsX</b>
  <br/>
  NSUInteger <b>primaryStrideInPixelsY</b>
  <br/>
  NSUInteger <b>secondaryStrideInPixelsX</b>
  <br/>
  NSUInteger <b>secondaryStrideInPixelsY</b>
  <br/>
  NSUInteger <b>primaryDilationRateX</b>
  <br/>
  NSUInteger <b>primaryDilationRateY</b>
  <br/>
  NSUInteger <b>secondaryDilationRateX</b>
  <br/>
  NSUInteger <b>secondaryDilationRateY</b>
  <br/>
  BOOL <b>isBackwards</b>
  <br/>
  BOOL <b>isStateModified</b>
  <br/>
  id&lt; <b>MPSNNPadding</b> &gt; <b>padding</b>
  <br/>
  id&lt; MPSImageAllocator &gt; <b>destinationImageAllocator</b>
  <br/>
  <br/>
</p>
</section>
<section class="Ss">
<h2 class="Ss" id="Additional_Inherited_Members"><a class="permalink" href="#Additional_Inherited_Members">Additional
  Inherited Members</a></h2>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Detailed_Description"><a class="permalink" href="#Detailed_Description">Detailed
  Description</a></h1>
<p class="Pp">This depends on Metal.framework Describes a convolution neural
    network kernel. <b>A</b> <b>MPSCNNKernel</b> consumes two MPSImages, primary
    and secondary, and produces one <b>MPSImage</b>.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="Method_Documentation"><a class="permalink" href="#Method_Documentation">Method
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss">- (BOOL) appendBatchBarrier </h2>
<p class="Pp">Returns YES if the filter must be run over the entire batch before
    its results may be considered complete The <b>MPSNNGraph</b> may split
    batches into sub-batches to save memory. However, some filters, like batch
    statistics calculations, need to operate over the entire batch to calculate
    a valid result, in this case, the mean and variance per channel over the set
    of images.</p>
<p class="Pp">In such cases, the accumulated result is commonly stored in a
    <b>MPSState</b> containing a MTLBuffer. (MTLTextures may not be able to be
    read from and written to in the same filter on some devices.)
    -isResultStateReusedAcrossBatch is set to YES, so that the state is
    allocated once and passed in for each sub-batch and the filter accumulates
    its results into it, one sub-batch at a time. Note that sub-batches may
    frequently be as small as 1.</p>
<p class="Pp">Default: NO</p>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSImageDescriptor</b>*__nonnull)
  destinationImageDescriptorForSourceImages: (NSArray&lt; <b>MPSImage</b> * &gt;
  *__nonnull) sourceImages(NSArray&lt; <b>MPSState</b> * &gt; *__nullable)
  sourceStates</h2>
<p class="Pp">Get a suggested destination image descriptor for a source image
    Your application is certainly free to pass in any destinationImage it likes
    to encodeToCommandBuffer:sourceImage:destinationImage, within reason. This
    is the basic design for iOS 10. This method is therefore not required.</p>
<p class="Pp">However, calculating the <b>MPSImage</b> size and
    <b>MPSCNNBinaryKernel</b> properties for each filter can be tedious and
    complicated work, so this method is made available to automate the process.
    The application may modify the properties of the descriptor before a
    <b>MPSImage</b> is made from it, so long as the choice is sensible for the
    kernel in question. Please see individual kernel descriptions for
    restrictions.</p>
<p class="Pp">The expected timeline for use is as follows:</p>
<p class="Pp">1) This method is called: a) The default MPS padding calculation
    is applied. It uses the MPSNNPaddingMethod of the .padding property to
    provide a consistent addressing scheme over the graph. It creates the
    <b>MPSImageDescriptor</b> and adjusts the .offset property of the
    MPSNNKernel. When using a <b>MPSNNGraph</b>, the padding is set using the
    <b>MPSNNFilterNode</b> as a proxy.</p>
<p class="Pp">b) This method may be overridden by <b>MPSCNNBinaryKernel</b>
    subclass to achieve any customization appropriate to the object type.</p>
<p class="Pp">c) Source states are then applied in order. These may modify the
    descriptor and may update other object properties. See:
    -destinationImageDescriptorForSourceImages:sourceStates:
    forKernel:suggestedDescriptor: This is the typical way in which MPS may
    attempt to influence the operation of its kernels.</p>
<p class="Pp">d) If the .padding property has a custom padding policy method of
    the same name, it is called. Similarly, it may also adjust the descriptor
    and any <b>MPSCNNBinaryKernel</b> properties. This is the typical way in
    which your application may attempt to influence the operation of the MPS
    kernels.</p>
<p class="Pp">2) <b>A</b> result is returned from this method and the caller may
    further adjust the descriptor and kernel properties directly.</p>
<p class="Pp">3) The caller uses the descriptor to make a new <b>MPSImage</b> to
    use as the destination image for the -encode call in step 5.</p>
<p class="Pp">4) The caller calls
    -resultStateForSourceImage:sourceStates:destinationImage: to make any result
    states needed for the kernel. If there isn't one, it will return nil.
    <b>A</b> variant is available to return a temporary state instead.</p>
<p class="Pp">5) a -encode method is called to encode the kernel.</p>
<p class="Pp">The entire process 1-5 is more simply achieved by just calling an
    -encode... method that returns a <b>MPSImage</b> out the left hand sid of
    the method. Simpler still, use the <b>MPSNNGraph</b> to coordinate the
    entire process from end to end. Opportunities to influence the process are
    of course reduced, as (2) is no longer possible with either method. Your
    application may opt to use the five step method if it requires greater
    customization as described, or if it would like to estimate storage in
    advance based on the sum of MPSImageDescriptors before processing a graph.
    Storage estimation is done by using the <b>MPSImageDescriptor</b> to create
    a <b>MPSImage</b> (without passing it a texture), and then call
    -resourceSize. As long as the <b>MPSImage</b> is not used in an encode call
    and the .texture property is not invoked, the underlying MTLTexture is not
    created.</p>
<p class="Pp">No destination state or destination image is provided as an
    argument to this function because it is expected they will be made /
    configured after this is called. This method is expected to auto-configure
    important object properties that may be needed in the ensuing destination
    image and state creation steps.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>sourceImages</i> <b>A</b> array of source images that
  will be passed into the -encode call Since <b>MPSCNNBinaryKernel</b> is a
  binary kernel, it is an array of length 2.
<br/>
<i>sourceStates</i> An optional array of source states that will be passed into
  the -encode call</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent">an image descriptor allocated on the autorelease
  pool</div>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSImageBatch</b> * __nonnull) encodeBatchToCommandBuffer:
  (nonnull id&lt; MTLCommandBuffer &gt;) commandBuffer(<b>MPSImageBatch</b>
  *__nonnull) primaryImage(<b>MPSImageBatch</b> *__nonnull) secondaryImage</h2>
<p class="Pp">Encode a <b>MPSCNNKernel</b> into a command Buffer. Create
    textures to hold the results and return them. In the first iteration on this
    method, encodeBatchToCommandBuffer:sourceImage:destinationImage: some work
    was left for the developer to do in the form of correctly setting the offset
    property and sizing the result buffer. With the introduction of the padding
    policy (see padding property) the filter can do this work itself. If you
    would like to have some input into what sort of <b>MPSImage</b> (e.g.
    temporary vs. regular) or what size it is or where it is allocated, you may
    set the destinationImageAllocator to allocate the image yourself.</p>
<p class="Pp">This method uses the <b>MPSNNPadding</b> padding property to
    figure out how to size the result image and to set the offset property. See
    discussion in <b>MPSNeuralNetworkTypes.h</b>. All images in a batch must
    have <b>MPSImage.numberOfImages</b> = 1.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The command buffer
<br/>
<i>primaryImage</i> <b>A</b> MPSImages to use as the primary source images for
  the filter.
<br/>
<i>secondaryImage</i> <b>A</b> MPSImages to use as the secondary source images
  for the filter.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> <b>MPSImage</b> or <b>MPSTemporaryImage</b>
  allocated per the destinationImageAllocator containing the output of the
  graph. The returned image will be automatically released when the command
  buffer completes. If you want to keep it around for longer, retain the image.
  (ARC will do this for you if you use it later.)</div>
<p class="Pp">Reimplemented in <b>MPSCNNBatchNormalizationGradient</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (void) encodeBatchToCommandBuffer: (nonnull id&lt;
  MTLCommandBuffer &gt;) commandBuffer(<b>MPSImageBatch</b> *__nonnull)
  primaryImages(<b>MPSImageBatch</b> *__nonnull)
  secondaryImages(<b>MPSImageBatch</b> *__nonnull) destinationImages</h2>
<p class="Pp">Encode a <b>MPSCNNKernel</b> into a command Buffer. The operation
    shall proceed out-of-place. This is the older style of encode which reads
    the offset, doesn't change it, and ignores the padding method. Multiple
    images are processed concurrently. All images must have
    <b>MPSImage.numberOfImages</b> = 1.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> <b>A</b> valid MTLCommandBuffer to
  receive the encoded filter
<br/>
<i>primaryImages</i> An array of <b>MPSImage</b> objects containing the primary
  source images.
<br/>
<i>secondaryImages</i> An array <b>MPSImage</b> objects containing the secondary
  source images.
<br/>
<i>destinationImages</i> An array of <b>MPSImage</b> objects to contain the
  result images. destinationImages may not alias primarySourceImages or
  secondarySourceImages in any manner.</div>
<p class="Pp">Reimplemented in <b>MPSCNNBatchNormalizationGradient</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSImageBatch</b> * __nonnull) encodeBatchToCommandBuffer:
  (nonnull id&lt; MTLCommandBuffer &gt;) commandBuffer(<b>MPSImageBatch</b>
  *__nonnull) primaryImages(<b>MPSImageBatch</b> *__nonnull)
  secondaryImages(__autoreleasing <b>MPSStateBatch</b> *__nullable *__nonnull)
  outState(BOOL) isTemporary</h2>
<p class="Pp">Encode a <b>MPSCNNKernel</b> into a command Buffer. Create a
    texture and state to hold the results and return them. In the first
    iteration on this method,
    encodeToCommandBuffer:sourceImage:destinationState:destinationImage: some
    work was left for the developer to do in the form of correctly setting the
    offset property and sizing the result buffer. With the introduction of the
    padding policy (see padding property) the filter can do this work itself. If
    you would like to have some input into what sort of <b>MPSImage</b> (e.g.
    temporary vs. regular) or what size it is or where it is allocated, you may
    set the destinationImageAllocator to allocate the image yourself.</p>
<p class="Pp">This method uses the <b>MPSNNPadding</b> padding property to
    figure out how to size the result image and to set the offset property. See
    discussion in <b>MPSNeuralNetworkTypes.h</b>. All images in a batch must
    have <b>MPSImage.numberOfImages</b> = 1.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The command buffer
<br/>
<i>primaryImages</i> <b>A</b> <b>MPSImage</b> to use as the source images for
  the filter.
<br/>
<i>secondaryImages</i> <b>A</b> <b>MPSImage</b> to use as the source images for
  the filter.
<br/>
<i>outState</i> <b>A</b> new state object is returned here.
<br/>
<i>isTemporary</i> YES if the outState should be a temporary object</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> <b>MPSImage</b> or <b>MPSTemporaryImage</b>
  allocated per the destinationImageAllocator containing the output of the
  graph. The offset property will be adjusted to reflect the offset used during
  the encode. The returned image will be automatically released when the command
  buffer completes. If you want to keep it around for longer, retain the image.
  (ARC will do this for you if you use it later.)</div>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSImage</b> * __nonnull) encodeToCommandBuffer: (nonnull
  id&lt; MTLCommandBuffer &gt;) commandBuffer(<b>MPSImage</b> *__nonnull)
  primaryImage(<b>MPSImage</b> *__nonnull) secondaryImage</h2>
<p class="Pp">Encode a <b>MPSCNNKernel</b> into a command Buffer. Create a
    texture to hold the result and return it. In the first iteration on this
    method, encodeToCommandBuffer:sourceImage:destinationImage: some work was
    left for the developer to do in the form of correctly setting the offset
    property and sizing the result buffer. With the introduction of the padding
    policy (see padding property) the filter can do this work itself. If you
    would like to have some input into what sort of <b>MPSImage</b> (e.g.
    temporary vs. regular) or what size it is or where it is allocated, you may
    set the destinationImageAllocator to allocate the image yourself.</p>
<p class="Pp">This method uses the <b>MPSNNPadding</b> padding property to
    figure out how to size the result image and to set the offset property. See
    discussion in <b>MPSNeuralNetworkTypes.h</b>.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The command buffer
<br/>
<i>primaryImage</i> <b>A</b> MPSImages to use as the primary source images for
  the filter.
<br/>
<i>secondaryImage</i> <b>A</b> MPSImages to use as the secondary source images
  for the filter.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> <b>MPSImage</b> or <b>MPSTemporaryImage</b>
  allocated per the destinationImageAllocator containing the output of the
  graph. The returned image will be automatically released when the command
  buffer completes. If you want to keep it around for longer, retain the image.
  (ARC will do this for you if you use it later.)</div>
<p class="Pp">Reimplemented in <b>MPSCNNBatchNormalizationGradient</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (void) encodeToCommandBuffer: (nonnull id&lt; MTLCommandBuffer
  &gt;) commandBuffer(<b>MPSImage</b> *__nonnull) primaryImage(<b>MPSImage</b>
  *__nonnull) secondaryImage(<b>MPSImage</b> *__nonnull) destinationImage</h2>
<p class="Pp">Encode a <b>MPSCNNKernel</b> into a command Buffer. The operation
    shall proceed out-of-place. This is the older style of encode which reads
    the offset, doesn't change it, and ignores the padding method.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> <b>A</b> valid MTLCommandBuffer to
  receive the encoded filter
<br/>
<i>primaryImage</i> <b>A</b> valid <b>MPSImage</b> object containing the primary
  source image.
<br/>
<i>secondaryImage</i> <b>A</b> valid <b>MPSImage</b> object containing the
  secondary source image.
<br/>
<i>destinationImage</i> <b>A</b> valid <b>MPSImage</b> to be overwritten by
  result image. destinationImage may not alias primarySourceImage or
  secondarySourceImage.</div>
<p class="Pp">Reimplemented in <b>MPSCNNBatchNormalizationGradient</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSImage</b> * __nonnull) encodeToCommandBuffer: (nonnull
  id&lt; MTLCommandBuffer &gt;) commandBuffer(<b>MPSImage</b> *__nonnull)
  primaryImage(<b>MPSImage</b> *__nonnull) secondaryImage(__autoreleasing
  <b>MPSState</b> *__nullable *__nonnull) outState(BOOL) isTemporary</h2>
<p class="Pp">Encode a <b>MPSCNNKernel</b> into a command Buffer. Create a
    texture and state to hold the results and return them. In the first
    iteration on this method,
    encodeToCommandBuffer:sourceImage:destinationState:destinationImage: some
    work was left for the developer to do in the form of correctly setting the
    offset property and sizing the result buffer. With the introduction of the
    padding policy (see padding property) the filter can do this work itself. If
    you would like to have some input into what sort of <b>MPSImage</b> (e.g.
    temporary vs. regular) or what size it is or where it is allocated, you may
    set the destinationImageAllocator to allocate the image yourself.</p>
<p class="Pp">This method uses the <b>MPSNNPadding</b> padding property to
    figure out how to size the result image and to set the offset property. See
    discussion in <b>MPSNeuralNetworkTypes.h</b>. All images in a batch must
    have <b>MPSImage.numberOfImages</b> = 1.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The command buffer
<br/>
<i>primaryImage</i> <b>A</b> <b>MPSImage</b> to use as the source images for the
  filter.
<br/>
<i>secondaryImage</i> <b>A</b> <b>MPSImage</b> to use as the source images for
  the filter.
<br/>
<i>outState</i> The address of location to write the pointer to the result state
  of the operation
<br/>
<i>isTemporary</i> YES if the outState should be a temporary object</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> <b>MPSImage</b> or <b>MPSTemporaryImage</b>
  allocated per the destinationImageAllocator containing the output of the
  graph. The offset property will be adjusted to reflect the offset used during
  the encode. The returned image will be automatically released when the command
  buffer completes. If you want to keep it around for longer, retain the image.
  (ARC will do this for you if you use it later.)</div>
</section>
<section class="Ss">
<h2 class="Ss">- (nullable instancetype) <b>initWithCoder:</b> (NSCoder
  *__nonnull) aDecoder(nonnull id&lt; MTLDevice &gt;) device</h2>
<p class="Pp"><b>NSSecureCoding</b> compatability While the standard
    NSSecureCoding/NSCoding method -initWithCoder: should work, since the file
    can't know which device your data is allocated on, we have to guess and may
    guess incorrectly. To avoid that problem, use initWithCoder:device
  instead.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>aDecoder</i> The NSCoder subclass with your serialized
  <b>MPSKernel</b>
<br/>
<i>device</i> The MTLDevice on which to make the <b>MPSKernel</b></div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> new <b>MPSKernel</b> object, or nil if
  failure.</div>
<p class="Pp">Reimplemented from <b>MPSKernel</b>.</p>
<p class="Pp">Reimplemented in <b>MPSCNNConvolutionGradient</b>,
    <b>MPSCNNFullyConnectedGradient</b>, <b>MPSCNNGradientKernel</b>,
    <b>MPSCNNPoolingAverageGradient</b>, <b>MPSCNNPoolingMaxGradient</b>,
    <b>MPSCNNPoolingL2NormGradient</b>, <b>MPSCNNDilatedPoolingMaxGradient</b>,
    <b>MPSCNNSoftMaxGradient</b>, <b>MPSCNNLogSoftMaxGradient</b>,
    <b>MPSCNNCrossChannelNormalizationGradient</b>,
    <b>MPSCNNPoolingGradient</b>,
    <b>MPSCNNLocalContrastNormalizationGradient</b>,
    <b>MPSCNNBatchNormalizationGradient</b>,
    <b>MPSCNNBatchNormalizationStatisticsGradient</b>,
    <b>MPSCNNNeuronGradient</b>, <b>MPSCNNDropoutGradient</b>, and
    <b>MPSCNNSpatialNormalizationGradient</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (nonnull instancetype) initWithDevice: (nonnull id&lt;
  MTLDevice &gt;) device</h2>
<p class="Pp">Standard init with default properties per filter type</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>device</i> The device that the filter will be used on.
  May not be NULL.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> pointer to the newly initialized object. This
  will fail, returning nil if the device is not supported. Devices must be
  MTLFeatureSet_iOS_GPUFamily2_v1 or later.</div>
<p class="Pp">Reimplemented from <b>MPSKernel</b>.</p>
<p class="Pp">Reimplemented in <b>MPSCNNConvolutionGradient</b>,
    <b>MPSCNNFullyConnectedGradient</b>, <b>MPSCNNGradientKernel</b>,
    <b>MPSCNNSoftMaxGradient</b>, <b>MPSCNNLogSoftMaxGradient</b>,
    <b>MPSCNNPoolingGradient</b>,
    <b>MPSNNReduceFeatureChannelsAndWeightsSum</b>,
    <b>MPSCNNArithmeticGradient</b>, <b>MPSNNReduceBinary</b>,
    <b>MPSNNReduceFeatureChannelsAndWeightsMean</b>,
    <b>MPSCNNNeuronGradient</b>, <b>MPSCNNUpsamplingGradient</b>,
    <b>MPSCNNDropoutGradient</b>, <b>MPSCNNArithmetic</b>, <b>MPSCNNAdd</b>,
    <b>MPSCNNSubtract</b>, <b>MPSCNNMultiply</b>, and <b>MPSCNNDivide</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (BOOL) isResultStateReusedAcrossBatch </h2>
<p class="Pp">Returns YES if the same state is used for every operation in a
    batch If NO, then each image in a MPSImageBatch will need a corresponding
    (and different) state to go with it. Set to YES to avoid allocating
    redundant state in the case when the same state is used all the time.
    Default: NO</p>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSStateBatch</b> * __nullable)
  resultStateBatchForPrimaryImage: (<b>MPSImageBatch</b> *__nonnull)
  primaryImage(<b>MPSImageBatch</b> *__nonnull) secondaryImage(NSArray&lt;
  <b>MPSStateBatch</b> * &gt; *__nullable) sourceStates(<b>MPSImageBatch</b>
  *__nonnull) destinationImage</h2>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSState</b> * __nullable) resultStateForPrimaryImage:
  (<b>MPSImage</b> *__nonnull) primaryImage(<b>MPSImage</b> *__nonnull)
  secondaryImage(NSArray&lt; <b>MPSState</b> * &gt; *__nullable)
  sourceStates(<b>MPSImage</b> *__nonnull) destinationImage</h2>
<p class="Pp">Allocate a <b>MPSState</b> (subclass) to hold the results from a
    -encodeBatchToCommandBuffer... operation <b>A</b> graph may need to allocate
    storage up front before executing. This may be necessary to avoid using too
    much memory and to manage large batches. The function should allocate a
    <b>MPSState</b> object (if any) that will be produced by an -encode call
    with the indicated sourceImages and sourceStates inputs. Though the states
    can be further adjusted in the ensuing -encode call, the states should be
    initialized with all important data and all MTLResource storage allocated.
    The data stored in the MTLResource need not be initialized, unless the
    ensuing -encode call expects it to be.</p>
<p class="Pp">The MTLDevice used by the result is derived from the source image.
    The padding policy will be applied to the filter before this is called to
    give it the chance to configure any properties like
    <b>MPSCNNKernel.offset</b>.</p>
<p class="Pp">CAUTION: the result state should be made after the kernel
    properties are configured for the -encode call that will write to the state,
    and after -destinationImageDescriptorForSourceImages:sourceStates: is called
    (if it is called). Otherwise, behavior is undefined. Please see the
    description of -[<b>MPSCNNKernel</b>
    resultStateForSourceImage:sourceStates:destinationImage:] for more.</p>
<p class="Pp">Default: returns nil</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>primaryImage</i> The <b>MPSImage</b> consumed by the
  associated -encode call.
<br/>
<i>secondaryImage</i> The <b>MPSImage</b> consumed by the associated -encode
  call.
<br/>
<i>sourceStates</i> The list of MPSStates consumed by the associated -encode
  call, for a batch size of 1.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent">The list of states produced by the -encode call for batch
  size of 1. When the batch size is not 1, this function will be called
  repeatedly unless -isResultStateReusedAcrossBatch returns YES. If
  -isResultStateReusedAcrossBatch returns YES, then it will be called once per
  batch and the MPSStateBatch array will contain MPSStateBatch.length references
  to the same object.</div>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSStateBatch</b> * __nullable)
  temporaryResultStateBatchForCommandBuffer: (nonnull id&lt; MTLCommandBuffer
  &gt;) commandBuffer(<b>MPSImageBatch</b> *__nonnull)
  primaryImage(<b>MPSImageBatch</b> *__nonnull) secondaryImage(NSArray&lt;
  <b>MPSStateBatch</b> * &gt; *__nullable) sourceStates(<b>MPSImageBatch</b>
  *__nonnull) destinationImage</h2>
</section>
<section class="Ss">
<h2 class="Ss">- (<b>MPSState</b> * __nullable)
  temporaryResultStateForCommandBuffer: (nonnull id&lt; MTLCommandBuffer &gt;)
  commandBuffer(<b>MPSImage</b> *__nonnull) primaryImage(<b>MPSImage</b>
  *__nonnull) secondaryImage(NSArray&lt; <b>MPSState</b> * &gt; *__nullable)
  sourceStates(<b>MPSImage</b> *__nonnull) destinationImage</h2>
<p class="Pp">Allocate a temporary <b>MPSState</b> (subclass) to hold the
    results from a -encodeBatchToCommandBuffer... operation <b>A</b> graph may
    need to allocate storage up front before executing. This may be necessary to
    avoid using too much memory and to manage large batches. The function should
    allocate any <b>MPSState</b> objects that will be produced by an -encode
    call with the indicated sourceImages and sourceStates inputs. Though the
    states can be further adjusted in the ensuing -encode call, the states
    should be initialized with all important data and all MTLResource storage
    allocated. The data stored in the MTLResource need not be initialized,
    unless the ensuing -encode call expects it to be.</p>
<p class="Pp">The MTLDevice used by the result is derived from the command
    buffer. The padding policy will be applied to the filter before this is
    called to give it the chance to configure any properties like
    <b>MPSCNNKernel.offset</b>.</p>
<p class="Pp">CAUTION: the result state should be made after the kernel
    properties are configured for the -encode call that will write to the state,
    and after -destinationImageDescriptorForSourceImages:sourceStates: is called
    (if it is called). Otherwise, behavior is undefined. Please see the
    description of -[<b>MPSCNNKernel</b>
    resultStateForSourceImage:sourceStates:destinationImage] for more.</p>
<p class="Pp">Default: returns nil</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> The command buffer to allocate the
  temporary storage against The state will only be valid on this command buffer.
<br/>
<i>primaryImage</i> The <b>MPSImage</b> consumed by the associated -encode call.
<br/>
<i>secondaryImage</i> The <b>MPSImage</b> consumed by the associated -encode
  call.
<br/>
<i>sourceStates</i> The list of MPSStates consumed by the associated -encode
  call, for a batch size of 1.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent">The list of states produced by the -encode call for batch
  size of 1. When the batch size is not 1, this function will be called
  repeatedly unless -isResultStateReusedAcrossBatch returns YES. If
  -isResultStateReusedAcrossBatch returns YES, then it will be called once per
  batch and the MPSStateBatch array will contain MPSStateBatch.length references
  to the same object.</div>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Property_Documentation"><a class="permalink" href="#Property_Documentation">Property
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss">- clipRect [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">An optional clip rectangle to use when writing data. Only the
    pixels in the rectangle will be overwritten. <b>A</b> MTLRegion that
    indicates which part of the destination to overwrite. If the clipRect does
    not lie completely within the destination image, the intersection between
    clip rectangle and destination bounds is used. Default: MPSRectNoClip
    (<b>MPSKernel::MPSRectNoClip</b>) indicating the entire image.
    clipRect.origin.z is the index of starting destination image in batch
    processing mode. clipRect.size.depth is the number of images to process in
    batch processing mode.</p>
<p class="Pp">See Also: <b>MPSKernel clipRect</b></p>
</section>
<section class="Ss">
<h2 class="Ss">- destinationFeatureChannelOffset [read]<b>, [write]</b>,
  [nonatomic]<b>, [assign]</b></h2>
<p class="Pp">The number of channels in the destination <b>MPSImage</b> to skip
    before writing output. This is the starting offset into the destination
    image in the feature channel dimension at which destination data is written.
    This allows an application to pass a subset of all the channels in
    <b>MPSImage</b> as output of <b>MPSKernel</b>. E.g. Suppose <b>MPSImage</b>
    has 24 channels and a <b>MPSKernel</b> outputs 8 channels. If we want
    channels 8 to 15 of this <b>MPSImage</b> to be used as output, we can set
    destinationFeatureChannelOffset = 8. Note that this offset applies
    independently to each image when the <b>MPSImage</b> is a container for
    multiple images and the <b>MPSCNNKernel</b> is processing multiple images
    (clipRect.size.depth &gt; 1). The default value is 0 and any value specifed
    shall be a multiple of 4. If <b>MPSKernel</b> outputs N channels,
    destination image MUST have at least destinationFeatureChannelOffset + N
    channels. Using a destination image with insufficient number of feature
    channels result in an error. E.g. if the <b>MPSCNNConvolution</b> outputs 32
    channels, and destination has 64 channels, then it is an error to set
    destinationFeatureChannelOffset &gt; 32.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (id&lt;MPSImageAllocator&gt;) destinationImageAllocator
  [read]<b>, [write]</b>, [nonatomic]<b>, [retain]</b></h2>
<p class="Pp">Method to allocate the result image for
    -encodeToCommandBuffer:sourceImage: Default: <b>defaultAllocator
    (MPSTemporaryImage)</b></p>
</section>
<section class="Ss">
<h2 class="Ss">- isBackwards [read]<b>, [nonatomic]</b>, [assign]<b></b></h2>
<p class="Pp">YES if the filter operates backwards. This influences how
    strideInPixelsX/Y should be interpreted.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (BOOL) isStateModified [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
<p class="Pp">Returns true if the -encode call modifies the state object it
    accepts.</p>
</section>
<section class="Ss">
<h2 class="Ss">- padding [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">The padding method used by the filter This influences how
    strideInPixelsX/Y should be interpreted. Default:
    MPSNNPaddingMethodAlignCentered | MPSNNPaddingMethodAddRemainderToTopLeft |
    MPSNNPaddingMethodSizeSame Some object types (e.g.
    <b>MPSCNNFullyConnected</b>) may override this default with something
    appropriate to its operation.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (NSUInteger) primaryDilationRateX [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- (NSUInteger) primaryDilationRateY [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- primaryEdgeMode [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">The MPSImageEdgeMode to use when texture reads stray off the edge
    of the primary source image Most <b>MPSKernel</b> objects can read off the
    edge of the source image. This can happen because of a negative offset
    property, because the offset + clipRect.size is larger than the source image
    or because the filter looks at neighboring pixels, such as a Convolution
    filter. Default: MPSImageEdgeModeZero.</p>
<p class="Pp">See Also: <b>MPSKernelEdgeMode</b></p>
</section>
<section class="Ss">
<h2 class="Ss">- primaryKernelHeight [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
<p class="Pp">The height of the <b>MPSCNNBinaryKernel</b> filter window This is
    the vertical diameter of the region read by the filter for each result
    pixel. If the <b>MPSCNNKernel</b> does not have a filter window, then 1 will
    be returned.</p>
</section>
<section class="Ss">
<h2 class="Ss">- primaryKernelWidth [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
<p class="Pp">The width of the <b>MPSCNNBinaryKernel</b> filter window This is
    the horizontal diameter of the region read by the filter for each result
    pixel. If the <b>MPSCNNKernel</b> does not have a filter window, then 1 will
    be returned.</p>
</section>
<section class="Ss">
<h2 class="Ss">- primaryOffset [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">The position of the destination clip rectangle origin relative to
    the primary source buffer. The offset is defined to be the position of
    clipRect.origin in source coordinates. Default: {0,0,0}, indicating that the
    top left corners of the clipRect and primary source image align. offset.z is
    the index of starting source image in batch processing mode.</p>
<p class="Pp">See Also: <b>subsubsection_mpsoffset</b></p>
</section>
<section class="Ss">
<h2 class="Ss">- primarySourceFeatureChannelMaxCount [read]<b>, [write]</b>,
  [nonatomic]<b>, [assign]</b></h2>
<p class="Pp">The maximum number of channels in the primary source
    <b>MPSImage</b> to use Most filters can insert a slice operation into the
    filter for free. Use this to limit the size of the feature channel slice
    taken from the input image. If the value is too large, it is truncated to be
    the remaining size in the image after the sourceFeatureChannelOffset is
    taken into account. Default: ULONG_MAX</p>
</section>
<section class="Ss">
<h2 class="Ss">- primarySourceFeatureChannelOffset [read]<b>, [write]</b>,
  [nonatomic]<b>, [assign]</b></h2>
<p class="Pp">The number of channels in the primary source <b>MPSImage</b> to
    skip before reading the input. This is the starting offset into the primary
    source image in the feature channel dimension at which source data is read.
    Unit: feature channels This allows an application to read a subset of all
    the channels in <b>MPSImage</b> as input of <b>MPSKernel</b>. E.g. Suppose
    <b>MPSImage</b> has 24 channels and a <b>MPSKernel</b> needs to read 8
    channels. If we want channels 8 to 15 of this <b>MPSImage</b> to be used as
    input, we can set primarySourceFeatureChannelOffset = 8. Note that this
    offset applies independently to each image when the <b>MPSImage</b> is a
    container for multiple images and the <b>MPSCNNKernel</b> is processing
    multiple images (clipRect.size.depth &gt; 1). The default value is 0 and any
    value specifed shall be a multiple of 4. If <b>MPSKernel</b> inputs N
    channels, the source image MUST have at least
    primarySourceFeatureChannelOffset + N channels. Using a source image with
    insufficient number of feature channels will result in an error. E.g. if the
    <b>MPSCNNConvolution</b> inputs 32 channels, and the source has 64 channels,
    then it is an error to set primarySourceFeatureChannelOffset &gt; 32.</p>
</section>
<section class="Ss">
<h2 class="Ss">- primaryStrideInPixelsX [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">The downsampling (or upsampling if a backwards filter) factor in
    the horizontal dimension for the primary source image If the filter does not
    do up or downsampling, 1 is returned.</p>
</section>
<section class="Ss">
<h2 class="Ss">- primaryStrideInPixelsY [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">The downsampling (or upsampling if a backwards filter) factor in
    the vertical dimension for the primary source image If the filter does not
    do up or downsampling, 1 is returned.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (NSUInteger) secondaryDilationRateX [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- (NSUInteger) secondaryDilationRateY [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- secondaryEdgeMode [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">The MPSImageEdgeMode to use when texture reads stray off the edge
    of the primary source image Most <b>MPSKernel</b> objects can read off the
    edge of the source image. This can happen because of a negative offset
    property, because the offset + clipRect.size is larger than the source image
    or because the filter looks at neighboring pixels, such as a Convolution
    filter. Default: MPSImageEdgeModeZero.</p>
<p class="Pp">See Also: <b>MPSKernelEdgeMode</b></p>
</section>
<section class="Ss">
<h2 class="Ss">- (NSUInteger) secondaryKernelHeight [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- (NSUInteger) secondaryKernelWidth [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
</section>
<section class="Ss">
<h2 class="Ss">- secondaryOffset [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">The position of the destination clip rectangle origin relative to
    the secondary source buffer. The offset is defined to be the position of
    clipRect.origin in source coordinates. Default: {0,0,0}, indicating that the
    top left corners of the clipRect and secondary source image align. offset.z
    is the index of starting source image in batch processing mode.</p>
<p class="Pp">See Also: <b>subsubsection_mpsoffset</b></p>
</section>
<section class="Ss">
<h2 class="Ss">- secondarySourceFeatureChannelMaxCount [read]<b>, [write]</b>,
  [nonatomic]<b>, [assign]</b></h2>
<p class="Pp">The maximum number of channels in the secondary source
    <b>MPSImage</b> to use Most filters can insert a slice operation into the
    filter for free. Use this to limit the size of the feature channel slice
    taken from the input image. If the value is too large, it is truncated to be
    the remaining size in the image after the sourceFeatureChannelOffset is
    taken into account. Default: ULONG_MAX</p>
</section>
<section class="Ss">
<h2 class="Ss">- secondarySourceFeatureChannelOffset [read]<b>, [write]</b>,
  [nonatomic]<b>, [assign]</b></h2>
<p class="Pp">The number of channels in the secondary source <b>MPSImage</b> to
    skip before reading the input. This is the starting offset into the
    secondary source image in the feature channel dimension at which source data
    is read. Unit: feature channels This allows an application to read a subset
    of all the channels in <b>MPSImage</b> as input of <b>MPSKernel</b>. E.g.
    Suppose <b>MPSImage</b> has 24 channels and a <b>MPSKernel</b> needs to read
    8 channels. If we want channels 8 to 15 of this <b>MPSImage</b> to be used
    as input, we can set secondarySourceFeatureChannelOffset = 8. Note that this
    offset applies independently to each image when the <b>MPSImage</b> is a
    container for multiple images and the <b>MPSCNNKernel</b> is processing
    multiple images (clipRect.size.depth &gt; 1). The default value is 0 and any
    value specifed shall be a multiple of 4. If <b>MPSKernel</b> inputs N
    channels, the source image MUST have at least
    primarySourceFeatureChannelOffset + N channels. Using a source image with
    insufficient number of feature channels will result in an error. E.g. if the
    <b>MPSCNNConvolution</b> inputs 32 channels, and the source has 64 channels,
    then it is an error to set primarySourceFeatureChannelOffset &gt; 32.</p>
</section>
<section class="Ss">
<h2 class="Ss">- secondaryStrideInPixelsX [read]<b>, [write]</b>,
  [nonatomic]<b>, [assign]</b></h2>
<p class="Pp">The downsampling (or upsampling if a backwards filter) factor in
    the horizontal dimension for the secondary source image If the filter does
    not do up or downsampling, 1 is returned.</p>
</section>
<section class="Ss">
<h2 class="Ss">- secondaryStrideInPixelsY [read]<b>, [write]</b>,
  [nonatomic]<b>, [assign]</b></h2>
<p class="Pp">The downsampling (or upsampling if a backwards filter) factor in
    the vertical dimension for the secondary source image If the filter does not
    do up or downsampling, 1 is returned.</p>
<p class="Pp"></p>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Author"><a class="permalink" href="#Author">Author</a></h1>
<p class="Pp">Generated automatically by Doxygen for
    MetalPerformanceShaders.framework from the source code.</p>
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">Mon Jul 9 2018</td>
    <td class="foot-os">Version MetalPerformanceShaders-119.3</td>
  </tr>
</table>
</body>
</html>
