<!DOCTYPE html>
<html>
<!-- This is an automatically generated file.  Do not edit.
   -*- nroff -*-
 -->
<head>
  <meta charset="utf-8"/>
  <link rel="stylesheet" href="../style.css" type="text/css" media="all"/>
  <title>MPSCNNConvolutionGradient(3)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">MPSCNNConvolutionGradient(3)</td>
    <td class="head-vol">MetalPerformanceShaders.framework</td>
    <td class="head-rtitle">MPSCNNConvolutionGradient(3)</td>
  </tr>
</table>
<div class="manual-text">
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
<p class="Pp">MPSCNNConvolutionGradient</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<p class="Pp">#import &lt;MPSCNNConvolution.h&gt;</p>
<p class="Pp">Inherits <b>MPSCNNGradientKernel</b>.</p>
<p class="Pp">Inherited by <b>MPSCNNFullyConnectedGradient</b>.</p>
<section class="Ss">
<h2 class="Ss" id="Instance_Methods"><a class="permalink" href="#Instance_Methods">Instance
  Methods</a></h2>
<br/>
<p class="Pp">(nonnull instancetype) - <b>initWithDevice:weights:</b>
  <br/>
  (nullable instancetype) - <b>initWithCoder:device:</b>
  <br/>
  (nonnull instancetype) - <b>initWithDevice:</b>
  <br/>
  (void) - <b>reloadWeightsAndBiasesFromDataSource</b>
  <br/>
  (void) - <b>reloadWeightsAndBiasesWithCommandBuffer:state:</b>
  <br/>
  <br/>
</p>
</section>
<section class="Ss">
<h2 class="Ss" id="Properties"><a class="permalink" href="#Properties">Properties</a></h2>
<br/>
<p class="Pp">NSUInteger <b>sourceGradientFeatureChannels</b>
  <br/>
  NSUInteger <b>sourceImageFeatureChannels</b>
  <br/>
  NSUInteger <b>groups</b>
  <br/>
  NSUInteger <b>channelMultiplier</b>
  <br/>
  id&lt; <b>MPSCNNConvolutionDataSource</b> &gt; <b>dataSource</b>
  <br/>
  <b>MPSCNNConvolutionGradientOption</b> <b>gradientOption</b>
  <br/>
  BOOL <b>serializeWeightsAndBiases</b>
  <br/>
  <br/>
</p>
</section>
<section class="Ss">
<h2 class="Ss" id="Additional_Inherited_Members"><a class="permalink" href="#Additional_Inherited_Members">Additional
  Inherited Members</a></h2>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Detailed_Description"><a class="permalink" href="#Detailed_Description">Detailed
  Description</a></h1>
<p class="Pp">This depends on Metal.framework The
    <b>MPSCNNConvolutionGradient</b> implementents backward propagation of
    gradient i.e. it computes the gradient of loss function with respect input
    data of corresonding forward convolution and gradient of loss function with
    respect to weights and bias of corresponding convolution in forward
  pass.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="Gradient_with_respect_to_data"><a class="permalink" href="#Gradient_with_respect_to_data">Gradient
  with respect to data </a></h1>
<p class="Pp">Gradient with respect to input data of corresponding forward
    convolution will be written in destination image passed to encode call of
    <b>MPSCNNConvolutionGradient</b>. This step is similar to convolution
    transpose in that the strided convolution in forward pass become zero filled
    convolution in backward propagation of gradients. The difference between
    <b>MPSCNNConvolutionTranspose</b> and gradient wrt data is how the weights,
    that are provided by data source, are interpreted. <b>MPSCNNConvolution</b>
    and <b>MPSCNNConvolutionTranspose</b> interpret weights provided by data
    source as
    weights[outputFeatureChannels][kernelWidth][kernelHeight][inputFeatureChannels]
    whereas convoution gradient with respect to data interpret the weights as
    weights[inputFeatureChannels][kernelWidth][kernelHeight][outputFeatureChannels]
    i.e. weights are transposed in inputFeatureChannels/outputFeatureChannels
    dimension and also rotated 180 degress in spatial dimension</p>
<p class="Pp">User should use the same data source provider to initialize
    <b>MPSCNNConvolutionGradient</b> as is used to initialize corresponding
    forward <b>MPSCNNConvolution</b>. Implementation will do the
    transposition/shuffling needed. Thus, while the forward
    <b>MPSCNNConvolution</b> takes sourceImage of inputFeatureChannels and
    convolves it with
    Wt[outputFeatureChannels][kernelHeight][kernelWidth][inputFeatureChannels]
    to produce destinationImage of outputFeatureChannels, MPSConvolutionGradient
    takes sourceGradient of outputFeatureChannels which is out of previous layer
    (nomally neuron backward layer), convolves it with transposed and rotated
    weights and produces destinationGradient of inputFeatureChannels. If the
    user decide to double buffer data source provider i.e. different data source
    providers are passed to forward <b>MPSCNNConvolution</b> object and
    corresponding <b>MPSCNNConvolutionGradient</b> object, it is user
    responsibility to make sure both data source providers provide same
    weights/bias data and have same properties in convolution descriptor else
    behavior is undefined.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="Gradient_with_respect_to_weights_and_bias"><a class="permalink" href="#Gradient_with_respect_to_weights_and_bias">Gradient
  with respect to weights and bias </a></h1>
<p class="Pp">Gradient with respect to weights and bias are returned in
    <b>MPSCNNConvolutionGradientState</b> object to be used in weights update
    functions. If I denotes the input image to corresponding
    <b>MPSCNNConvolution</b> in forward pass and E denoates the loss gradient
    from previous layer (normally neuron backward layer) in backward pass,
    gradient of E with respect to weights is</p>
<p class="Pp">delta_E/delta_Wkpqc = sum_i sum_j [ E(i - primaryOffset.x,j -
    primaryOffset.y, k) * I( secondaryStrideInPixelX*i + secondaryOffset.x -
    secondaryDilationRateX*secondaryKernelWidth/2 + secondaryDilationRateX*p,
    secondaryStrideinPixelY*i + secondaryOffset.y -
    secondaryDilationRateY*secondaryKernelHeight/2 + secondaryDilationRateY*q,
    c) ]</p>
<p class="Pp">where i goes over 0..W-1 and j goes over 0..H-1, (W,H) being width
    and height of E. p in [0, secondaryKernelWidth-1] q in [0,
    secondaryKernelHeight-1] c in [0, inputeFeatureChannels/groups - 1] k in [0,
    outputFeatureChannels]</p>
<p class="Pp">and gradient with respect to bias</p>
<p class="Pp">delta_E/delta_bk = sum_i sum_j [ E(i - primaryOffset.x,j -
    primaryOffset.y, k) ]</p>
<p class="Pp">These gradients with respect to weights and bias are returned as
    buffers in <b>MPSCNNConvolutionGradientState</b> object passed in the encode
    call. These are consumed by <b>MPSCNNConvolution</b> object's
    -updateWeightsAndBias:MPSCNNConvolutionGradientState* method for CPU side
    update and
    encodeWeightsAndBiasUpdate:commandBuffer:MPSCNNConvolutionGradientState*
    method of <b>MPSCNNConvolution</b> object for GPU side update. UPdated
    weights and biases are computed as</p>
<p class="Pp"></p>
<pre>
<br/>
       Wkpqc_new = Wkpqc_old + delta_E/delta_Wkpqc
<br/>
       bk_new = bk_old + delta_E/delta_bk
</pre>
<p class="Pp">Note that <b>MPSCNNConvolutionGradientState</b> objects's buffers
    that contain gradients, for CPU side update, will only contain valid data
    after command buffer is complete so its only makes sense to call
    -updateWeightsAndBias method on <b>MPSCNNConvolution</b> objects after
    command bufer is complete. One can achieve this by enqueueing a command
    buffer completion handler block that make this call. Since
    <b>MPSCNNConvolutionGradientState</b> is used across command buffers i.e.
    its created in forward pass, consumed by <b>MPSCNNConvolutionGradient</b> in
    backward pass in same command buffer and passed onto
    <b>MPSCNNConvolution</b> updateWeightsAndBias method after completion of
    command buffer, it cannot be a temporary state.</p>
<p class="Pp">In order to gaurantee consistency between forward pass
    (<b>MPSCNNConvolution</b>) and weights gradient computation in this filter,
    certain requirements must be met. 1) Dimensions of loss gradient E from
    previous layer in backward pass must be equal to clipRect.size of
    corresponding <b>MPSCNNConvolution</b> in forward pass. This is to gaurantee
    that only those pixels for which weights/bias contributed in destination of
    forward pass end up contributing to weights/bias gradient update. If the
    dimension of loss gradient E from previous layer is not equal to
    clipRect.size of corresponding forward <b>MPSCNNConvolution</b>, i) one can
    insert a slice operation to extract out the region of size clipRect.size
    from appropriate offset in E and set primaryOffset = 0 Or ii) set
    primatryOffset to offset in E at which valid data starts and make sure data
    outside is zeroed. 2) secondaryOffset should be set to what offset property
    of <b>MPSCNNConvolution</b> was set to in forward pass.</p>
<p class="Pp">Currently back propagation for gradients is only supported for
    regualar convolution and depthwise convolution. Back propagation sub-pixel
    convolution are not supported. So channelMultiplier and subPixelScaleFactor
    must be one.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="Method_Documentation"><a class="permalink" href="#Method_Documentation">Method
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss">- (nullable instancetype) <b>initWithCoder:</b> (NSCoder
  *__nonnull) aDecoder(nonnull id&lt; MTLDevice &gt;) device</h2>
<p class="Pp"><b>NSSecureCoding</b> compatability While the standard
    NSSecureCoding/NSCoding method -initWithCoder: should work, since the file
    can't know which device your data is allocated on, we have to guess and may
    guess incorrectly. To avoid that problem, use initWithCoder:device
  instead.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>aDecoder</i> The NSCoder subclass with your serialized
  <b>MPSKernel</b>
<br/>
<i>device</i> The MTLDevice on which to make the <b>MPSKernel</b></div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> new <b>MPSKernel</b> object, or nil if
  failure.</div>
<p class="Pp">Reimplemented from <b>MPSCNNGradientKernel</b>.</p>
<p class="Pp">Reimplemented in <b>MPSCNNFullyConnectedGradient</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (nonnull instancetype) initWithDevice: (nonnull id&lt;
  MTLDevice &gt;) device</h2>
<p class="Pp">Standard init with default properties per filter type</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>device</i> The device that the filter will be used on.
  May not be NULL.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> pointer to the newly initialized object. This
  will fail, returning nil if the device is not supported. Devices must be
  MTLFeatureSet_iOS_GPUFamily2_v1 or later.</div>
<p class="Pp">Reimplemented from <b>MPSCNNGradientKernel</b>.</p>
<p class="Pp">Reimplemented in <b>MPSCNNFullyConnectedGradient</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (nonnull instancetype) <b>initWithDevice:</b> (nonnull id&lt;
  MTLDevice &gt;) device(nonnull id&lt; <b>MPSCNNConvolutionDataSource</b> &gt;)
  weights</h2>
<p class="Pp">Initializes a convolution gradient (with respect to weights and
    bias) object.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>device</i> The MTLDevice on which this
  <b>MPSCNNConvolutionGradient</b> filter will be used
<br/>
<i>weights</i> <b>A</b> pointer to a object that conforms to the
  <b>MPSCNNConvolutionDataSource</b> protocol. Note that same data source as
  provided to forward convolution should be used.</div>
<p class="Pp"><b>Returns:</b></p>
<div class="Bd-indent"><b>A</b> valid <b>MPSCNNConvolutionGradient</b> object or
  nil, if failure.</div>
<p class="Pp">Reimplemented in <b>MPSCNNFullyConnectedGradient</b>.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (void) reloadWeightsAndBiasesFromDataSource </h2>
<p class="Pp">CPU side reload. Reload the updated weights and biases from data
    provider into internal weights and bias buffers. Weights and biases
    gradients needed for update are obtained from
    <b>MPSCNNConvolutionGradientState</b> object. Data provider passed in init
    call is used for this purpose.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (void) reloadWeightsAndBiasesWithCommandBuffer: (__nonnull
  id&lt; MTLCommandBuffer &gt;)
  commandBuffer(<b>MPSCNNConvolutionWeightsAndBiasesState</b> *__nonnull)
  state</h2>
<p class="Pp">GPU side reload. Reload the updated weights and biases from update
    buffer produced by application enqueued metal kernel into internal weights
    and biases buffer. Weights and biases gradients needed for update are
    obtained from <b>MPSCNNConvolutionGradientState</b> object's
    gradientForWeights and gradientForBiases metal buffer.</p>
<p class="Pp"><b>Parameters:</b></p>
<div class="Bd-indent"><i>commandBuffer</i> Metal command buffer on which
  application update kernel was enqueued consuming
  <b>MPSCNNConvolutionGradientState</b>'s gradientForWeights and
  gradientForBiases buffer and producing updateBuffer metal buffer.
<br/>
<i>state</i> <b>MPSCNNConvolutionWeightsAndBiasesState</b> containing weights
  and biases buffers which have updated weights produced by application's update
  kernel.</div>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Property_Documentation"><a class="permalink" href="#Property_Documentation">Property
  Documentation</a></h1>
<section class="Ss">
<h2 class="Ss">- (NSUInteger) channelMultiplier [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
<p class="Pp">Channel multiplier. For convolution created with
    <b>MPSCNNDepthWiseConvolutionDescriptor</b>, it is the number of output
    feature channels for each input channel. See
    <b>MPSCNNDepthWiseConvolutionDescriptor</b> for more details. Default is 0
    which means regular CNN convolution. Currently only channelMultiplier of 1
    is supported i.e. inputChannels == outputChannels</p>
</section>
<section class="Ss">
<h2 class="Ss">- dataSource [read]<b>, [nonatomic]</b>, [retain]<b></b></h2>
<p class="Pp">dataSource with which gradient object was created</p>
</section>
<section class="Ss">
<h2 class="Ss">- gradientOption [read]<b>, [write]</b>, [nonatomic]<b>,
  [assign]</b></h2>
<p class="Pp">Option to control which gradient to compute. Default is
    MPSCNNConvolutionGradientOptionAll which means both gradient with respect to
    data and gradient with respect to weight and bias are computed.</p>
</section>
<section class="Ss">
<h2 class="Ss">- groups [read]<b>, [nonatomic]</b>, [assign]<b></b></h2>
<p class="Pp">Number of groups input and output channels are divided into.</p>
</section>
<section class="Ss">
<h2 class="Ss">- (BOOL) serializeWeightsAndBiases [read]<b>, [write]</b>,
  [nonatomic]<b>, [assign]</b></h2>
<p class="Pp">Property to control serialization of weights and bias. During
    serialization of convolution object in -encodeWithCoder call, weights and
    biases are saved so that convolution object can be properly
    unserialized/restored in -initWithCoder call. If data source provied is
    <b>NSSecureCoding</b> compliant, data source is serialized else weights and
    biases are serialized. As weights/biases data may be several MB and these
    are same for both gradient and forward convolution object, application may
    already have weights/biases on disk through convolution, it can save disk
    space by setting this property false so convolution gradient object does not
    end up storing another copy of weights/biases. Default is NO. When
    application decides to set it to NO, it MUST call -(void)
    reloadWeightsAndBiasesFromDataSource after initWithCoder has initialized
    convolution object.</p>
</section>
<section class="Ss">
<h2 class="Ss">- sourceGradientFeatureChannels [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
<p class="Pp">The number of feature channels per pixel in the gradient image
    (primarySource) of encode call. This is same is outputFeatureChannels or the
    feature channels of destination image in forward convolution i.e.
    dataSource.descriptor.outputFeatureChannels</p>
</section>
<section class="Ss">
<h2 class="Ss">- sourceImageFeatureChannels [read]<b>, [nonatomic]</b>,
  [assign]<b></b></h2>
<p class="Pp">The number of feature channels per pixel in the input image to
    forward convolution which is used here as secondarySource. This is same as
    dataSource.descriptor.inputFeatureChannels. This is also the number of
    feature channels in destinatin image here i.e. gradient with respect to
    data.</p>
<p class="Pp"></p>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="Author"><a class="permalink" href="#Author">Author</a></h1>
<p class="Pp">Generated automatically by Doxygen for
    MetalPerformanceShaders.framework from the source code.</p>
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">Mon Jul 9 2018</td>
    <td class="foot-os">Version MetalPerformanceShaders-119.3</td>
  </tr>
</table>
</body>
</html>
