<!DOCTYPE html>
<html lang="en">
<!-- This is an automatically generated file.  Do not edit.
   Automatically generated by Pod::Man 4.14 (Pod::Simple 3.42)
  
   Standard preamble:
   ========================================================================
   Vertical space (when we can't use .PP)
 -->
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <link rel="stylesheet" href="../style.css" type="text/css" media="all"/>
  <title>PPI::Tokenizer(3)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">PPI::Tokenizer(3)</td>
    <td class="head-vol"><a href=".">User Contributed Perl Documentation</a></td>
    <td class="head-rtitle">PPI::Tokenizer(3)</td>
  </tr>
</table>
<div class="manual-text">
<br/>
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
<p class="Pp">PPI::Tokenizer - The Perl Document Tokenizer</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<pre>  # Create a tokenizer for a file, array or string
  $Tokenizer = PPI::Tokenizer-&gt;new( 'filename.pl' );
  $Tokenizer = PPI::Tokenizer-&gt;new( \@lines       );
  $Tokenizer = PPI::Tokenizer-&gt;new( \$source      );
  
  # Return all the tokens for the document
  my $tokens = $Tokenizer-&gt;all_tokens;
  
  # Or we can use it as an iterator
  while ( my $Token = $Tokenizer-&gt;get_token ) {
        print &quot;Found token '$Token'\n&quot;;
  }
  
  # If we REALLY need to manually nudge the cursor, you
  # can do that to (The lexer needs this ability to do rollbacks)
  $is_incremented = $Tokenizer-&gt;increment_cursor;
  $is_decremented = $Tokenizer-&gt;decrement_cursor;
</pre>
</section>
<section class="Sh">
<h1 class="Sh" id="DESCRIPTION"><a class="permalink" href="#DESCRIPTION">DESCRIPTION</a></h1>
<p class="Pp">PPI::Tokenizer is the class that provides Tokenizer objects for
    use in breaking strings of Perl source code into Tokens.</p>
<p class="Pp">By the time you are reading this, you probably need to know a
    little about the difference between how perl parses Perl &quot;code&quot;
    and how PPI parsers Perl &quot;documents&quot;.</p>
<p class="Pp">&quot;perl&quot; itself (the interpreter) uses a heavily modified
    lex specification to specify its parsing logic, maintains several types of
    state as it goes, and incrementally tokenizes, lexes AND EXECUTES at the
    same time.</p>
<p class="Pp">In fact, it is provably impossible to use perl's parsing method
    without simultaneously executing code. A formal mathematical proof has been
    published demonstrating the method.</p>
<p class="Pp">This is where the truism &quot;Only perl can parse Perl&quot;
    comes from.</p>
<p class="Pp">PPI uses a completely different approach by abandoning the
    (impossible) ability to parse Perl the same way that the interpreter does,
    and instead parsing the source as a document, using a document structure
    independently derived from the Perl documentation and approximating the perl
    interpreter interpretation as closely as possible.</p>
<p class="Pp">It was touch and go for a long time whether we could get it close
    enough, but in the end it turned out that it could be done.</p>
<p class="Pp">In this approach, the tokenizer
    <span class="Li">&quot;PPI::Tokenizer&quot;</span> is implemented separately
    from the lexer PPI::Lexer.</p>
<p class="Pp">The job of <span class="Li">&quot;PPI::Tokenizer&quot;</span> is
    to take pure source as a string and break it up into a stream/set of tokens,
    and contains most of the &quot;black magic&quot; used in PPI. By comparison,
    the lexer implements a relatively straight forward tree structure, and has
    an implementation that is uncomplicated (compared to the insanity in the
    tokenizer at least).</p>
<p class="Pp">The Tokenizer uses an immense amount of heuristics, guessing and
    cruft, supported by a very <b>VERY</b> flexible internal API, but
    fortunately it was possible to largely encapsulate the black magic, so there
    is not a lot that gets exposed to people using the
    <span class="Li">&quot;PPI::Tokenizer&quot;</span> itself.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="METHODS"><a class="permalink" href="#METHODS">METHODS</a></h1>
<p class="Pp">Despite the incredible complexity, the Tokenizer itself only
    exposes a relatively small number of methods, with most of the complexity
    implemented in private methods.</p>
<section class="Ss">
<h2 class="Ss" id="new_$file___"><a class="permalink" href="#new_$file___">new
  $file | \@lines | \$source</a></h2>
<p class="Pp">The main <span class="Li">&quot;new&quot;</span> constructor
    creates a new Tokenizer object. These objects have no configuration
    parameters, and can only be used once, to tokenize a single perl source
    file.</p>
<p class="Pp">It takes as argument either a normal scalar containing source
    code, a reference to a scalar containing source code, or a reference to an
    ARRAY containing newline-terminated lines of source code.</p>
<p class="Pp">Returns a new <span class="Li">&quot;PPI::Tokenizer&quot;</span>
    object on success, or throws a PPI::Exception exception on error.</p>
</section>
<section class="Ss">
<h2 class="Ss" id="get_token"><a class="permalink" href="#get_token">get_token</a></h2>
<p class="Pp">When using the PPI::Tokenizer object as an iterator, the
    <span class="Li">&quot;get_token&quot;</span> method is the primary method
    that is used. It increments the cursor and returns the next Token in the
    output array.</p>
<p class="Pp">The actual parsing of the file is done only as-needed, and a line
    at a time. When <span class="Li">&quot;get_token&quot;</span> hits the end
    of the token array, it will cause the parser to pull in the next line and
    parse it, continuing as needed until there are more tokens on the output
    array that get_token can then return.</p>
<p class="Pp">This means that a number of Tokenizer objects can be created, and
    won't consume significant CPU until you actually begin to pull tokens from
    it.</p>
<p class="Pp">Return a PPI::Token object on success, <span class="Li">0</span>
    if the Tokenizer had reached the end of the file, or
    <span class="Li">&quot;undef&quot;</span> on error.</p>
</section>
<section class="Ss">
<h2 class="Ss" id="all_tokens"><a class="permalink" href="#all_tokens">all_tokens</a></h2>
<p class="Pp">When not being used as an iterator, the
    <span class="Li">&quot;all_tokens&quot;</span> method tells the Tokenizer to
    parse the entire file and return all of the tokens in a single ARRAY
    reference.</p>
<p class="Pp">It should be noted that
    <span class="Li">&quot;all_tokens&quot;</span> does <b>NOT</b> interfere
    with the use of the Tokenizer object as an iterator (does not modify the
    token cursor) and use of the two different mechanisms can be mixed
  safely.</p>
<p class="Pp">Returns a reference to an ARRAY of PPI::Token objects on success
    or throws an exception on error.</p>
</section>
<section class="Ss">
<h2 class="Ss" id="increment_cursor"><a class="permalink" href="#increment_cursor">increment_cursor</a></h2>
<p class="Pp">Although exposed as a public method,
    <span class="Li">&quot;increment_cursor&quot;</span> is implemented for
    expert use only, when writing lexers or other components that work directly
    on token streams.</p>
<p class="Pp">It manually increments the token cursor forward through the file,
    in effect &quot;skipping&quot; the next token.</p>
<p class="Pp">Return true if the cursor is incremented,
    <span class="Li">0</span> if already at the end of the file, or
    <span class="Li">&quot;undef&quot;</span> on error.</p>
</section>
<section class="Ss">
<h2 class="Ss" id="decrement_cursor"><a class="permalink" href="#decrement_cursor">decrement_cursor</a></h2>
<p class="Pp">Although exposed as a public method,
    <span class="Li">&quot;decrement_cursor&quot;</span> is implemented for
    expert use only, when writing lexers or other components that work directly
    on token streams.</p>
<p class="Pp">It manually decrements the token cursor backwards through the
    file, in effect &quot;rolling back&quot; the token stream. And indeed that
    is what it is primarily intended for, when the component that is consuming
    the token stream needs to implement some sort of &quot;roll back&quot;
    feature in its use of the token stream.</p>
<p class="Pp">Return true if the cursor is decremented,
    <span class="Li">0</span> if already at the beginning of the file, or
    <span class="Li">&quot;undef&quot;</span> on error.</p>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="NOTES"><a class="permalink" href="#NOTES">NOTES</a></h1>
<section class="Ss">
<h2 class="Ss" id="How_the_Tokenizer_Works"><a class="permalink" href="#How_the_Tokenizer_Works">How
  the Tokenizer Works</a></h2>
<p class="Pp">Understanding the Tokenizer is not for the faint-hearted. It is by
    far the most complex and twisty piece of perl I've ever written that is
    actually still built properly and isn't a terrible spaghetti-like mess. In
    fact, you probably want to skip this section.</p>
<p class="Pp">But if you really want to understand, well then here goes.</p>
</section>
<section class="Ss">
<h2 class="Ss" id="Source_Input_and_Clean_Up"><a class="permalink" href="#Source_Input_and_Clean_Up">Source
  Input and Clean Up</a></h2>
<p class="Pp">The Tokenizer starts by taking source in a variety of forms,
    sucking it all in and merging into one big string, and doing our own
    internal line split, using a &quot;universal line separator&quot; which
    allows the Tokenizer to take source for any platform (and even supports a
    few known types of broken newlines caused by mixed mac/pc/*nix editor screw
    ups).</p>
<p class="Pp">The resulting array of lines is used to feed the tokenizer, and is
    also accessed directly by the heredoc-logic to do the line-oriented part of
    here-doc support.</p>
</section>
<section class="Ss">
<h2 class="Ss" id="Doing_Things_the_Old_Fashioned_Way"><a class="permalink" href="#Doing_Things_the_Old_Fashioned_Way">Doing
  Things the Old Fashioned Way</a></h2>
<p class="Pp">Due to the complexity of perl, and after 2 previously aborted
    parser attempts, in the end the tokenizer was fashioned around a
    line-buffered character-by-character method.</p>
<p class="Pp">That is, the Tokenizer pulls and holds a line at a time into a
    line buffer, and then iterates a cursor along it. At each cursor position, a
    method is called in whatever token class we are currently in, which will
    examine the character at the current position, and handle it.</p>
<p class="Pp">As the handler methods in the various token classes are called,
    they build up an output token array for the source code.</p>
<p class="Pp">Various parts of the Tokenizer use look-ahead, arbitrary-distance
    look-behind (although currently the maximum is three significant tokens), or
    both, and various other heuristic guesses.</p>
<p class="Pp">I've been told it is officially termed a <i>&quot;backtracking
    parser</i> <i>with infinite lookaheads&quot;</i>.</p>
</section>
<section class="Ss">
<h2 class="Ss" id="State_Variables"><a class="permalink" href="#State_Variables">State
  Variables</a></h2>
<p class="Pp">Aside from the current line and the character cursor, the
    Tokenizer maintains a number of different state variables.</p>
<dl class="Bl-tag">
  <dt id="Current"><a class="permalink" href="#Current">Current Class</a></dt>
  <dd>The Tokenizer maintains the current token class at all times. Much of the
      time is just going to be the &quot;Whitespace&quot; class, which is what
      the base of a document is. As the tokenizer executes the various character
      handlers, the class changes a lot as it moves a long. In fact, in some
      instances, the character handler may not handle the character directly
      itself, but rather change the &quot;current class&quot; and then hand off
      to the character handler for the new class.
    <p class="Pp">Because of this, and some other things I'll deal with later,
        the number of times the character handlers are called does not in fact
        have a direct relationship to the number of actual characters in the
        document.</p>
  </dd>
  <dt id="Current~2"><a class="permalink" href="#Current~2">Current
    Zone</a></dt>
  <dd>Rather than create a class stack to allow for infinitely nested layers of
      classes, the Tokenizer recognises just a single layer.
    <p class="Pp">To put it a different way, in various parts of the file, the
        Tokenizer will recognise different &quot;base&quot; or
        &quot;substrate&quot; classes. When a Token such as a comment or a
        number is finalised by the tokenizer, it &quot;falls back&quot; to the
        base state.</p>
    <p class="Pp">This allows proper tokenization of special areas such as
        __DATA__ and __END__ blocks, which also contain things like comments and
        POD, without allowing the creation of any significant Tokens inside
        these areas.</p>
    <p class="Pp">For the main part of a document we use PPI::Token::Whitespace
        for this, with the idea being that code is &quot;floating in a sea of
        whitespace&quot;.</p>
  </dd>
  <dt id="Current~3"><a class="permalink" href="#Current~3">Current
    Token</a></dt>
  <dd>The final main state variable is the &quot;current token&quot;. This is
      the Token that is currently being built by the Tokenizer. For certain
      types, it can be manipulated and morphed and change class quite a bit
      while being assembled, as the Tokenizer's understanding of the token
      content changes.
    <p class="Pp">When the Tokenizer is confident that it has seen the end of
        the Token, it will be &quot;finalized&quot;, which adds it to the output
        token array and resets the current class to that of the zone that we are
        currently in.</p>
    <p class="Pp">I should also note at this point that the &quot;current
        token&quot; variable is optional. The Tokenizer is capable of knowing
        what class it is currently set to, without actually having accumulated
        any characters in the Token.</p>
  </dd>
</dl>
</section>
<section class="Ss">
<h2 class="Ss" id="Making_It_Faster"><a class="permalink" href="#Making_It_Faster">Making
  It Faster</a></h2>
<p class="Pp">As I'm sure you can imagine, calling several different methods for
    each character and running regexes and other complex heuristics made the
    first fully working version of the tokenizer extremely slow.</p>
<p class="Pp">During testing, I created a metric to measure parsing speed called
    LPGC, or &quot;lines per gigacycle&quot; . A gigacycle is simple a billion
    CPU cycles on a typical single-core CPU, and so a Tokenizer running at
    &quot;1000 lines per gigacycle&quot; should generate around 1200 lines of
    tokenized code when running on a 1200 MHz processor.</p>
<p class="Pp">The first working version of the tokenizer ran at only 350 LPGC,
    so to tokenize a typical large module such as ExtUtils::MakeMaker took 10-15
    seconds. This sluggishness made it unpractical for many uses.</p>
<p class="Pp">So in the current parser, there are multiple layers of
    optimisation very carefully built in to the basic. This has brought the
    tokenizer up to a more reasonable 1000 LPGC, at the expense of making the
    code quite a bit twistier.</p>
</section>
<section class="Ss">
<h2 class="Ss">Making It Faster - Whole Line Classification</h2>
<p class="Pp">The first step in the optimisation process was to add a hew
    handler to enable several of the more basic classes (whitespace, comments)
    to be able to be parsed a line at a time. At the start of each line, a
    special optional handler (only supported by a few classes) is called to
    check and see if the entire line can be parsed in one go.</p>
<p class="Pp">This is used mainly to handle things like POD, comments, empty
    lines, and a few other minor special cases.</p>
</section>
<section class="Ss">
<h2 class="Ss">Making It Faster - Inlining</h2>
<p class="Pp">The second stage of the optimisation involved inlining a small
    number of critical methods that were repeated an extremely high number of
    times. Profiling suggested that there were about 1,000,000 individual method
    calls per gigacycle, and by cutting these by two thirds a significant speed
    improvement was gained, in the order of about 50%.</p>
<p class="Pp">You may notice that many methods in the
    <span class="Li">&quot;PPI::Tokenizer&quot;</span> code look very nested and
    long hand. This is primarily due to this inlining.</p>
<p class="Pp">At around this time, some statistics code that existed in the
    early versions of the parser was also removed, as it was determined that it
    was consuming around 15% of the CPU for the entire parser, while making the
    core more complicated.</p>
<p class="Pp">A judgment call was made that with the difficulties likely to be
    encountered with future planned enhancements, and given the relatively high
    cost involved, the statistics features would be removed from the
  Tokenizer.</p>
</section>
<section class="Ss">
<h2 class="Ss">Making It Faster - Quote Engine</h2>
<p class="Pp">Once inlining had reached diminishing returns, it became obvious
    from the profiling results that a huge amount of time was being spent
    stepping a char at a time though long, simple and &quot;syntactically
    boring&quot; code such as comments and strings.</p>
<p class="Pp">The existing regex engine was expanded to also encompass quotes
    and other quote-like things, and a special abstract base class was added
    that provided a number of specialised parsing methods that would &quot;scan
    ahead&quot;, looking out ahead to find the end of a string, and updating the
    cursor to leave it in a valid position for the next call.</p>
<p class="Pp">This is also the point at which the number of character handler
    calls began to greatly differ from the number of characters. But it has been
    done in a way that allows the parser to retain the power of the original
    version at the critical points, while skipping through the &quot;boring
    bits&quot; as needed for additional speed.</p>
<p class="Pp">The addition of this feature allowed the tokenizer to exceed 1000
    LPGC for the first time.</p>
</section>
<section class="Ss">
<h2 class="Ss">Making It Faster - The &quot;Complete&quot; Mechanism</h2>
<p class="Pp">As it became evident that great speed increases were available by
    using this &quot;skipping ahead&quot; mechanism, a new handler method was
    added that explicitly handles the parsing of an entire token, where the
    structure of the token is relatively simple. Tokens such as symbols fit this
    case, as once we are passed the initial sigil and word char, we know that we
    can skip ahead and &quot;complete&quot; the rest of the token much more
    easily.</p>
<p class="Pp">A number of these have been added for most or possibly all of the
    common cases, with most of these &quot;complete&quot; handlers implemented
    using regular expressions.</p>
<p class="Pp">In fact, so many have been added that at this point, you could
    arguably reclassify the tokenizer as a &quot;hybrid regex, char-by=char
    heuristic tokenizer&quot;. More tokens are now consumed in
    &quot;complete&quot; methods in a typical program than are handled by the
    normal char-by-char methods.</p>
<p class="Pp">Many of the these complete-handlers were implemented during the
    writing of the Lexer, and this has allowed the full parser to maintain
    around 1000 LPGC despite the increasing weight of the Lexer.</p>
</section>
<section class="Ss">
<h2 class="Ss">Making It Faster - Porting To C (In Progress)</h2>
<p class="Pp">While it would be extraordinarily difficult to port all of the
    Tokenizer to C, work has started on a PPI::XS &quot;accelerator&quot;
    package which acts as a separate and automatically-detected add-on to the
    main PPI package.</p>
<p class="Pp">PPI::XS implements faster versions of a variety of functions
    scattered over the entire PPI codebase, from the Tokenizer Core, Quote
    Engine, and various other places, and implements them identically in
  XS/C.</p>
<p class="Pp">In particular, the skip-ahead methods from the Quote Engine would
    appear to be extremely amenable to being done in C, and a number of other
    functions could be cherry-picked one at a time and implemented in C.</p>
<p class="Pp">Each method is heavily tested to ensure that the functionality is
    identical, and a versioning mechanism is included to ensure that if a
    function gets out of sync, PPI::XS will degrade gracefully and just not
    replace that single method.</p>
</section>
</section>
<section class="Sh">
<h1 class="Sh" id="TO_DO"><a class="permalink" href="#TO_DO">TO DO</a></h1>
<p class="Pp">- Add an option to reset or seek the token stream...</p>
<p class="Pp">- Implement more Tokenizer functions in PPI::XS</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SUPPORT"><a class="permalink" href="#SUPPORT">SUPPORT</a></h1>
<p class="Pp">See the support section in the main module.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="AUTHOR"><a class="permalink" href="#AUTHOR">AUTHOR</a></h1>
<p class="Pp">Adam Kennedy &lt;adamk@cpan.org&gt;</p>
</section>
<section class="Sh">
<h1 class="Sh" id="COPYRIGHT"><a class="permalink" href="#COPYRIGHT">COPYRIGHT</a></h1>
<p class="Pp">Copyright 2001 - 2011 Adam Kennedy.</p>
<p class="Pp">This program is free software; you can redistribute it and/or
    modify it under the same terms as Perl itself.</p>
<p class="Pp">The full text of the license can be found in the LICENSE file
    included with this module.</p>
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">2019-07-09</td>
    <td class="foot-os"><a href="..">perl v5.34.0</a></td>
  </tr>
</table>
</body>
</html>
