<!DOCTYPE html>
<html lang="en">
<!-- This is an automatically generated file.  Do not edit.
   Automatically generated by Pod::Man 4.14 (Pod::Simple 3.42)
  
   Standard preamble:
   ========================================================================
   Vertical space (when we can't use .PP)
 -->
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <link rel="stylesheet" href="../style.css" type="text/css" media="all"/>
  <title>WWW::RobotRules(3)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">WWW::RobotRules(3)</td>
    <td class="head-vol"><a href=".">User Contributed Perl Documentation</a></td>
    <td class="head-rtitle">WWW::RobotRules(3)</td>
  </tr>
</table>
<div class="manual-text">
<br/>
<section class="Sh">
<h1 class="Sh" id="NAME"><a class="permalink" href="#NAME">NAME</a></h1>
<p class="Pp">WWW::RobotRules - database of robots.txt-derived permissions</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SYNOPSIS"><a class="permalink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<pre> use WWW::RobotRules;
 my $rules = WWW::RobotRules-&gt;new('MOMspider/1.0');
 use LWP::Simple qw(get);
 {
   my $url = &quot;http://some.place/robots.txt&quot;;
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }
 {
   my $url = &quot;http://some.other.place/robots.txt&quot;;
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }
 # Now we can check if a URL is valid for those servers
 # whose &quot;robots.txt&quot; files we've gotten and parsed:
 if($rules-&gt;allowed($url)) {
     $c = get $url;
     ...
 }
</pre>
</section>
<section class="Sh">
<h1 class="Sh" id="DESCRIPTION"><a class="permalink" href="#DESCRIPTION">DESCRIPTION</a></h1>
<p class="Pp">This module parses <i>/robots.txt</i> files as specified in
    &quot;A Standard for Robot Exclusion&quot;, at
    &lt;http://www.robotstxt.org/wc/norobots.html&gt; Webmasters can use the
    <i>/robots.txt</i> file to forbid conforming robots from accessing parts of
    their web site.</p>
<p class="Pp">The parsed files are kept in a WWW::RobotRules object, and this
    object provides methods to check if access to a given URL is prohibited. The
    same WWW::RobotRules object can be used for one or more parsed
    <i>/robots.txt</i> files on any number of hosts.</p>
<p class="Pp">The following methods are provided:</p>
<dl class="Bl-tag">
  <dt>$rules = WWW::RobotRules-&gt;new($robot_name)</dt>
  <dd>This is the constructor for WWW::RobotRules objects. The first argument
      given to <b>new()</b> is the name of the robot.</dd>
  <dt>$rules-&gt;parse($robot_txt_url, $content, $fresh_until)</dt>
  <dd>The <b>parse()</b> method takes as arguments the URL that was used to
      retrieve the <i>/robots.txt</i> file, and the contents of the file.</dd>
  <dt>$rules-&gt;allowed($uri)</dt>
  <dd>Returns TRUE if this robot is allowed to retrieve this URL.</dd>
  <dt>$rules-&gt;agent([$name])</dt>
  <dd>Get/set the agent name. NOTE: Changing the agent name will clear the
      robots.txt rules and expire times out of the cache.</dd>
</dl>
</section>
<section class="Sh">
<h1 class="Sh" id="ROBOTS.TXT"><a class="permalink" href="#ROBOTS.TXT">ROBOTS.TXT</a></h1>
<p class="Pp">The format and semantics of the &quot;/robots.txt&quot; file are
    as follows (this is an edited abstract of
    &lt;http://www.robotstxt.org/wc/norobots.html&gt;):</p>
<p class="Pp">The file consists of one or more records separated by one or more
    blank lines. Each record contains lines of the form</p>
<p class="Pp"></p>
<pre>  &lt;field-name&gt;: &lt;value&gt;
</pre>
<p class="Pp">The field name is case insensitive. Text after the '#' character
    on a line is ignored during parsing. This is used for comments. The
    following &lt;field-names&gt; can be used:</p>
<dl class="Bl-tag">
  <dt id="User-Agent"><a class="permalink" href="#User-Agent">User-Agent</a></dt>
  <dd>The value of this field is the name of the robot the record is describing
      access policy for. If more than one <i>User-Agent</i> field is present the
      record describes an identical access policy for more than one robot. At
      least one field needs to be present per record. If the value is '*', the
      record describes the default access policy for any robot that has not not
      matched any of the other records.
    <p class="Pp">The <i>User-Agent</i> fields must occur before the
        <i>Disallow</i> fields. If a record contains a <i>User-Agent</i> field
        after a <i>Disallow</i> field, that constitutes a malformed record. This
        parser will assume that a blank line should have been placed before that
        <i>User-Agent</i> field, and will break the record into two. All the
        fields before the <i>User-Agent</i> field will constitute a record, and
        the <i>User-Agent</i> field will be the first field in a new record.</p>
  </dd>
  <dt id="Disallow"><a class="permalink" href="#Disallow">Disallow</a></dt>
  <dd>The value of this field specifies a partial URL that is not to be visited.
      This can be a full path, or a partial path; any URL that starts with this
      value will not be retrieved</dd>
</dl>
<p class="Pp">Unrecognized records are ignored.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="ROBOTS.TXT_EXAMPLES"><a class="permalink" href="#ROBOTS.TXT_EXAMPLES">ROBOTS.TXT
  EXAMPLES</a></h1>
<p class="Pp">The following example &quot;/robots.txt&quot; file specifies that
    no robots should visit any URL starting with &quot;/cyberworld/map/&quot; or
    &quot;/tmp/&quot;:</p>
<p class="Pp"></p>
<pre>  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  Disallow: /tmp/ # these will soon disappear
</pre>
<p class="Pp">This example &quot;/robots.txt&quot; file specifies that no robots
    should visit any URL starting with &quot;/cyberworld/map/&quot;, except the
    robot called &quot;cybermapper&quot;:</p>
<p class="Pp"></p>
<pre>  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  # Cybermapper knows where to go.
  User-agent: cybermapper
  Disallow:
</pre>
<p class="Pp">This example indicates that no robots should visit this site
    further:</p>
<p class="Pp"></p>
<pre>  # go away
  User-agent: *
  Disallow: /
</pre>
<p class="Pp">This is an example of a malformed robots.txt file.</p>
<p class="Pp"></p>
<pre>  # robots.txt for ancientcastle.example.com
  # I've locked myself away.
  User-agent: *
  Disallow: /
  # The castle is your home now, so you can go anywhere you like.
  User-agent: Belle
  Disallow: /west-wing/ # except the west wing!
  # It's good to be the Prince...
  User-agent: Beast
  Disallow:
</pre>
<p class="Pp">This file is missing the required blank lines between records.
    However, the intention is clear.</p>
</section>
<section class="Sh">
<h1 class="Sh" id="SEE_ALSO"><a class="permalink" href="#SEE_ALSO">SEE
  ALSO</a></h1>
<p class="Pp">LWP::RobotUA, WWW::RobotRules::AnyDBM_File</p>
</section>
<section class="Sh">
<h1 class="Sh" id="COPYRIGHT"><a class="permalink" href="#COPYRIGHT">COPYRIGHT</a></h1>
<pre>  Copyright 1995-2009, Gisle Aas
  Copyright 1995, Martijn Koster
</pre>
<p class="Pp">This library is free software; you can redistribute it and/or
    modify it under the same terms as Perl itself.</p>
</section>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">2012-02-18</td>
    <td class="foot-os"><a href="..">perl v5.34.0</a></td>
  </tr>
</table>
</body>
</html>
